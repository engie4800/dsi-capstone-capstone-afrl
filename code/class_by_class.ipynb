{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split, DataLoader, SubsetRandomSampler, Subset, ConcatDataset\n",
    "import os\n",
    "from Truncate import truncate\n",
    "from BSI_Entropy import BSIE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: .\\cifar10.tgz\n"
     ]
    }
   ],
   "source": [
    "# Download the 60000 Cifar-10 images\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "download_url(dataset_url, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder named data to hold our 50000 training images and 10000 testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "    tar.extractall(path='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cifar-10 dataset is extracted to the directory data/cifar10. It contains 2 folders train and test, containing the training set (50000 images) and test set (10000 images) respectively. Each of them contains 10 folders, one for each class of images. Let's verify this using os.listdir (list the names of directories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/cifar10'\n",
    "\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are an equal number of images for each class, 5000 for each class in the training set and 1000 for each class in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples for airplanes class: 5000\n"
     ]
    }
   ],
   "source": [
    "airplane_images = os.listdir(data_dir + \"/train/airplane\")\n",
    "print('Number of training examples for airplanes class:', len(airplane_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples for ship class: 1000\n"
     ]
    }
   ],
   "source": [
    "ship_test_images = os.listdir(data_dir + \"/test/ship\")\n",
    "print(\"Number of test examples for ship class:\", len(ship_test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the ImageFolder class from torchvision to load the image data as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display one image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1490, 0.1647, 0.1843,  ..., 0.3373, 0.2941, 0.2235],\n",
       "         [0.0902, 0.0784, 0.0941,  ..., 0.3333, 0.2745, 0.2039],\n",
       "         [0.1922, 0.1569, 0.1765,  ..., 0.3294, 0.2667, 0.1922],\n",
       "         ...,\n",
       "         [0.4902, 0.4980, 0.5137,  ..., 0.4000, 0.3333, 0.2627],\n",
       "         [0.3922, 0.4000, 0.4196,  ..., 0.4392, 0.4275, 0.4078],\n",
       "         [0.3216, 0.3255, 0.3451,  ..., 0.3961, 0.4039, 0.4157]],\n",
       "\n",
       "        [[0.2824, 0.3059, 0.3294,  ..., 0.4431, 0.4078, 0.3216],\n",
       "         [0.2235, 0.2314, 0.2431,  ..., 0.4392, 0.3882, 0.2902],\n",
       "         [0.3569, 0.3373, 0.3373,  ..., 0.4353, 0.3686, 0.2706],\n",
       "         ...,\n",
       "         [0.4392, 0.4314, 0.4510,  ..., 0.3451, 0.2902, 0.2353],\n",
       "         [0.3608, 0.3647, 0.3843,  ..., 0.3843, 0.3804, 0.3608],\n",
       "         [0.3098, 0.3176, 0.3333,  ..., 0.3569, 0.3569, 0.3569]],\n",
       "\n",
       "        [[0.5490, 0.5725, 0.5882,  ..., 0.7176, 0.6706, 0.6157],\n",
       "         [0.5059, 0.5137, 0.5137,  ..., 0.7137, 0.6549, 0.5765],\n",
       "         [0.6627, 0.6392, 0.6353,  ..., 0.7137, 0.6471, 0.5451],\n",
       "         ...,\n",
       "         [0.3922, 0.3922, 0.4078,  ..., 0.3176, 0.2667, 0.2157],\n",
       "         [0.3216, 0.3294, 0.3490,  ..., 0.3529, 0.3529, 0.3373],\n",
       "         [0.2824, 0.2902, 0.3020,  ..., 0.3216, 0.3255, 0.3294]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = training_dataset[49399]\n",
    "print(label)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same reason here, we load our testing image data as PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following section, we load in our pretrained CNN model serving as our embedding function to extract feature space from the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)) # this is the final CNN layer before classification prediction \n",
    "                                # (the one produce M features in Mr.Diggans' letter) and in this case M = 10\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('cifar10-cnn.pth')) # load our pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cifar10CnnModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.9372484087944031, 'val_acc': 0.7689453363418579}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size=128\n",
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After loading in our embedding function, we can test whether it can extract the intermediate feature successfully by inputting one image from our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2820e848dd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network[18].register_forward_hook(get_features('18'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = training_dataset[36666]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(to_device(img.unsqueeze(0), device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-17.1250,  -2.6508, -20.5190, -21.7576,  -6.1672,  -4.2082, -18.6081,\n",
       "          -8.5854,  -4.3932, -19.4672,  -4.1187, -17.1577, -13.7157, -11.8042,\n",
       "          -6.0299, -16.6095,  33.7439,  -5.3955, -10.6941, -16.5395,  10.8451,\n",
       "         -12.4619,  26.5413, -10.2965, -11.3364,  12.2312,  -4.2819,  18.3797,\n",
       "          -8.7769,  -8.9047, -22.7475, -24.0030,  -3.2923,  -8.8365,  19.1856,\n",
       "          -8.4078,   0.9128,  -6.7034,  -4.5636, -11.3492,  -4.9906,  -4.7368,\n",
       "          -6.1834,  -6.2606,  -9.2778,  -7.9591,  -8.0501, -12.8337,  -6.1143,\n",
       "         -13.8880,  -8.8909,  -4.9458,  -5.0991,  -6.1092,  26.6735, -12.8832,\n",
       "         -16.6240, -14.6230,  22.4230,  -7.9574, -14.2497, -11.5065,  -6.9942,\n",
       "          -9.9396]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we can establish the feature space for each class in Cifar-10 training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "class_feature_data = {label: [] for label in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in training_dataset:\n",
    "    if label in class_feature_data:\n",
    "        output = model(to_device(image.unsqueeze(0), device))\n",
    "        class_feature_data[label].append(features['18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in class_feature_data:\n",
    "    class_feature_data[label] = torch.cat(class_feature_data[label], dim = 0).T \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can further find the singular vector and entropy value for feature space of each training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_vector = {label: torch.svd(class_feature_data[label])[1] for label in class_feature_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: tensor([8305.8789, 1585.3182, 1073.5406,  963.0099,  828.8642,  769.5621,\n",
       "          616.4266,  580.3412,  516.1616,  488.6954,  402.4275,  341.2084,\n",
       "          275.5750,  266.5753,  257.8827,  231.7271,  198.1219,  183.4829,\n",
       "          180.9243,  177.5941,  156.9813,  144.6703,  141.9810,  132.6933,\n",
       "          126.5861,  122.6644,  117.9465,  113.8699,  110.0822,  106.3363,\n",
       "          103.7124,   93.6782,   91.7617,   89.5256,   84.1956,   82.4179,\n",
       "           80.3859,   78.2227,   75.4202,   72.8308,   71.0707,   69.9203,\n",
       "           65.3629,   64.8393,   62.6921,   60.4378,   58.1620,   56.9152,\n",
       "           54.7706,   54.1711,   52.7214,   51.6815,   50.4430,   49.4335,\n",
       "           47.0124,   45.7740,   43.8802,   43.6093,   42.2480,   40.5315,\n",
       "           39.1501,   37.6497,   36.0807,   34.7374]),\n",
       " 1: tensor([12095.2061,  1403.4337,  1103.4655,   963.6672,   940.5552,   853.0527,\n",
       "           744.2352,   670.6514,   567.7023,   496.4117,   435.3126,   407.3419,\n",
       "           350.2057,   277.2058,   250.3987,   228.0301,   213.1301,   201.9636,\n",
       "           183.1754,   177.0799,   173.2148,   157.2500,   154.9059,   141.8093,\n",
       "           136.7393,   130.5036,   123.9009,   111.7951,   109.5792,   104.0679,\n",
       "           102.0208,    98.5726,    95.6230,    92.5422,    91.4762,    87.7978,\n",
       "            85.6393,    80.0061,    75.8346,    74.1482,    72.9685,    70.6738,\n",
       "            68.0645,    67.4477,    63.2545,    62.6573,    58.7270,    57.1137,\n",
       "            55.9109,    54.9257,    53.4217,    51.6753,    50.8225,    49.1771,\n",
       "            46.7251,    46.5821,    44.0645,    43.7546,    42.5789,    39.9152,\n",
       "            38.9562,    36.2970,    35.4175,    31.8563]),\n",
       " 2: tensor([6268.6768,  996.6603,  861.0262,  768.5312,  739.1451,  694.6696,\n",
       "          600.1634,  527.4907,  457.0351,  414.4943,  364.2594,  286.4704,\n",
       "          252.5366,  232.2138,  215.6590,  201.4022,  193.2244,  170.9879,\n",
       "          168.6637,  152.4425,  139.6423,  135.0280,  128.0127,  120.3685,\n",
       "          113.6636,  107.9870,  103.6013,  101.6421,   95.3172,   93.9559,\n",
       "           88.8051,   85.6524,   82.2905,   80.0169,   76.2634,   74.8454,\n",
       "           73.6839,   72.4649,   67.7688,   66.6625,   64.4223,   61.4686,\n",
       "           60.3447,   58.9813,   57.0994,   55.9028,   54.6610,   53.0541,\n",
       "           49.8968,   48.1705,   46.9777,   46.7670,   45.8985,   43.3955,\n",
       "           42.3475,   41.3480,   40.6217,   39.2341,   37.1279,   36.3377,\n",
       "           35.2792,   34.6068,   33.2892,   31.7638]),\n",
       " 3: tensor([6473.5073,  841.2786,  778.4031,  699.7131,  636.0625,  580.7430,\n",
       "          545.3973,  478.3984,  441.1371,  360.8887,  271.3305,  227.0016,\n",
       "          215.7848,  210.0773,  184.9863,  174.0165,  161.9877,  152.2511,\n",
       "          149.6021,  136.9679,  129.8981,  125.9189,  120.4214,  109.9679,\n",
       "          104.4016,  103.5947,   97.8948,   96.5096,   85.8014,   85.5063,\n",
       "           82.8391,   81.4511,   78.8962,   73.4542,   70.5081,   69.6882,\n",
       "           66.7658,   64.2578,   64.0274,   62.4119,   60.9270,   60.1115,\n",
       "           58.6312,   55.4811,   53.9831,   52.1251,   51.3327,   50.8141,\n",
       "           47.4976,   47.1265,   45.3364,   43.8578,   43.6088,   43.3220,\n",
       "           41.2991,   39.3377,   38.7576,   37.6581,   36.6142,   35.8798,\n",
       "           34.7457,   33.8410,   33.0287,   31.0110]),\n",
       " 4: tensor([7251.3999, 1030.9064,  909.2327,  707.2688,  637.4417,  578.5201,\n",
       "          508.1140,  466.0776,  446.0780,  397.8710,  320.4633,  269.8743,\n",
       "          264.0945,  229.7420,  226.3317,  194.3986,  188.6454,  166.8791,\n",
       "          154.9358,  152.1550,  140.9037,  124.5961,  121.1879,  118.2018,\n",
       "          111.4068,   99.5754,   99.0052,   96.0552,   93.1947,   88.4500,\n",
       "           83.6878,   81.2279,   77.7753,   77.0465,   75.4441,   72.9830,\n",
       "           69.8479,   68.6878,   66.5030,   65.3280,   62.9284,   60.9607,\n",
       "           57.7178,   55.7312,   55.0482,   51.6707,   50.9145,   48.9035,\n",
       "           48.0556,   47.2219,   46.6511,   44.7408,   42.6484,   41.6211,\n",
       "           40.9727,   39.4311,   38.1823,   37.1305,   35.6565,   34.5898,\n",
       "           33.4765,   32.3365,   31.2384,   30.4821]),\n",
       " 5: tensor([6615.5000,  935.8938,  749.0901,  681.4392,  659.0427,  591.3130,\n",
       "          568.2397,  471.9457,  434.5263,  397.9953,  320.4688,  229.8735,\n",
       "          207.7418,  199.3160,  181.2436,  179.3377,  164.5392,  158.5162,\n",
       "          144.6888,  135.9444,  125.6181,  122.4719,  114.7564,  108.4020,\n",
       "           98.2476,   96.3082,   94.3113,   87.3923,   86.2062,   83.2604,\n",
       "           80.6292,   78.4679,   76.5878,   72.7462,   71.2571,   69.3191,\n",
       "           67.5854,   64.7256,   61.7663,   59.2221,   57.9550,   57.0776,\n",
       "           55.5528,   53.5544,   52.0715,   51.1490,   49.4429,   48.2574,\n",
       "           47.1759,   45.2133,   44.9031,   42.5675,   41.7146,   40.3162,\n",
       "           39.2917,   38.8730,   38.4060,   36.5054,   34.4352,   33.8537,\n",
       "           32.6440,   32.3013,   31.1622,   29.4608]),\n",
       " 6: tensor([7264.2148, 1025.9622,  821.9323,  766.3347,  668.6025,  559.7619,\n",
       "          490.6772,  444.9507,  408.4172,  347.1433,  305.5564,  228.6729,\n",
       "          222.3128,  206.2740,  182.9453,  170.8316,  157.5761,  153.2848,\n",
       "          131.4534,  127.3159,  126.6383,  111.9518,  103.7711,  100.5675,\n",
       "           94.8066,   89.5136,   87.9663,   82.4812,   80.3807,   75.5823,\n",
       "           75.0115,   72.9168,   70.3978,   67.0040,   66.0912,   62.3881,\n",
       "           61.8958,   60.0585,   58.2270,   55.7178,   53.8061,   52.5783,\n",
       "           49.9245,   48.5092,   48.1664,   46.0542,   45.5001,   43.5996,\n",
       "           41.0822,   40.6236,   39.1936,   38.3900,   36.9234,   36.2563,\n",
       "           35.1064,   34.7974,   33.9532,   31.8801,   31.6158,   30.7459,\n",
       "           29.4570,   27.8902,   26.1976,   25.0586]),\n",
       " 7: tensor([9169.8828, 1493.6215, 1051.6710,  827.7892,  704.5131,  648.9139,\n",
       "          604.9308,  552.1254,  529.1401,  405.0483,  379.9564,  326.4211,\n",
       "          275.0515,  241.4179,  225.5518,  223.2711,  190.9410,  185.7654,\n",
       "          180.1633,  159.7839,  149.9531,  140.9736,  129.3146,  123.8428,\n",
       "          118.9251,  117.3516,  110.0507,  101.6755,  100.0895,   94.0084,\n",
       "           89.3221,   87.6476,   83.3122,   83.0177,   79.0291,   77.2495,\n",
       "           76.2627,   75.2523,   72.6804,   71.6437,   68.6938,   65.9932,\n",
       "           62.9971,   61.2922,   59.8471,   58.7325,   57.2511,   55.7114,\n",
       "           54.3630,   52.0995,   49.9387,   48.9176,   47.1538,   46.9003,\n",
       "           44.8975,   43.6781,   41.0143,   40.4583,   40.1294,   39.0184,\n",
       "           37.9315,   34.9119,   34.0490,   32.1201]),\n",
       " 8: tensor([9594.4805, 1124.0610, 1052.3362, 1004.5804,  869.7374,  671.5615,\n",
       "          621.2519,  523.7562,  475.3379,  443.6017,  403.6830,  317.1657,\n",
       "          286.7527,  249.1192,  224.5342,  216.5424,  185.4713,  184.0177,\n",
       "          174.7412,  168.6416,  144.3980,  139.9327,  130.4194,  128.2969,\n",
       "          119.3904,  116.7353,  110.4874,  104.6957,  102.1369,  100.8237,\n",
       "           93.3471,   87.9545,   87.0240,   81.4927,   76.3620,   74.8747,\n",
       "           72.8455,   72.1757,   69.9687,   67.7925,   63.9059,   63.6517,\n",
       "           61.2246,   57.5016,   57.1501,   54.6067,   54.3347,   52.6592,\n",
       "           51.7210,   50.4249,   48.6767,   46.9900,   45.1177,   43.7656,\n",
       "           41.3862,   39.9052,   39.1436,   38.6954,   36.9722,   35.7993,\n",
       "           34.9810,   33.8001,   32.5454,   30.0804]),\n",
       " 9: tensor([10423.7295,  1075.6924,  1005.2132,   868.2537,   845.2270,   768.0860,\n",
       "           653.5539,   533.2513,   448.2159,   377.9797,   349.4870,   321.1977,\n",
       "           258.0855,   248.5007,   222.5318,   209.2140,   185.0927,   176.9587,\n",
       "           170.2491,   150.6953,   141.7572,   133.9512,   126.0533,   120.6464,\n",
       "           114.3579,   106.9467,   101.3006,   100.5582,    96.6379,    94.3995,\n",
       "            91.8883,    90.4309,    87.2283,    83.8863,    80.6998,    77.5528,\n",
       "            76.5992,    75.3139,    74.1166,    70.6445,    68.6330,    63.5477,\n",
       "            61.1679,    60.1646,    56.9889,    55.9821,    54.9749,    54.2968,\n",
       "            52.9092,    51.9405,    50.1667,    48.8075,    46.8235,    46.0916,\n",
       "            45.4860,    44.6703,    41.4841,    40.2458,    37.5727,    36.9767,\n",
       "            36.2387,    35.1060,    33.5660,    30.2874])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singular_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7522338164816981, 1: 0.7164786375096308, 2: 0.7705121005378595, 3: 0.7549243843017072, 4: 0.7589535094709319, 5: 0.7425858237466618, 6: 0.7425237755562969, 7: 0.7394834090879348, 8: 0.7432928614572687, 9: 0.727277486877264}\n"
     ]
    }
   ],
   "source": [
    "entropy = {label: BSIE(truncate(singular_vector[label], 3), [0, float('nan')]).item() for label in singular_vector}\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can load in the training dataset of 50000 class by class below and store all of them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: (0, 5000), 1:(5000, 10000), 2:(10000, 15000), 3:(15000, 20000), 4:(20000, 25000), 5:(25000, 30000), 6:(30000, 35000), 7:(35000, 40000), 8:(40000, 45000), 9:(45000, 50000)}\n",
    "training_data_by_class = {category: Subset(ImageFolder('./data/cifar10/train/', transform=ToTensor()), range(classes[category][0], classes[category][1])) for category in classes}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By default, we select 500 images from each class in our training data and combine them to train a new CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_subsample(classified_training_data, n_sample = 500): \n",
    "    subsample = {i: random_split(classified_training_data[i], [n_sample, len(classified_training_data[i]) - n_sample])[0] for i in classified_training_data}\n",
    "    return subsample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample_500 = select_subsample(training_data_by_class)\n",
    "len(subsample_500[7]) # that is, we have 500 image-label pairs in class 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3529, 0.3490, 0.3647,  ..., 0.3490, 0.3608, 0.5216],\n",
       "         [0.3529, 0.3529, 0.3686,  ..., 0.3490, 0.3608, 0.4667],\n",
       "         [0.3725, 0.3647, 0.3765,  ..., 0.3608, 0.3686, 0.4510],\n",
       "         ...,\n",
       "         [0.6353, 0.6118, 0.6118,  ..., 0.6196, 0.6078, 0.6510],\n",
       "         [0.7961, 0.7765, 0.7804,  ..., 0.7686, 0.7686, 0.7843],\n",
       "         [0.9098, 0.9059, 0.9059,  ..., 0.9020, 0.9020, 0.9098]],\n",
       "\n",
       "        [[0.5882, 0.6000, 0.6039,  ..., 0.6078, 0.5961, 0.6510],\n",
       "         [0.5922, 0.6078, 0.6118,  ..., 0.6118, 0.6000, 0.5961],\n",
       "         [0.6118, 0.6196, 0.6275,  ..., 0.6235, 0.6118, 0.5882],\n",
       "         ...,\n",
       "         [0.6353, 0.6118, 0.6118,  ..., 0.6196, 0.6078, 0.6510],\n",
       "         [0.7961, 0.7765, 0.7804,  ..., 0.7686, 0.7686, 0.7843],\n",
       "         [0.9098, 0.9059, 0.9059,  ..., 0.9020, 0.9020, 0.9098]],\n",
       "\n",
       "        [[0.7529, 0.7647, 0.7529,  ..., 0.7765, 0.7569, 0.7333],\n",
       "         [0.7569, 0.7686, 0.7608,  ..., 0.7804, 0.7608, 0.6863],\n",
       "         [0.7765, 0.7843, 0.7725,  ..., 0.7882, 0.7765, 0.6824],\n",
       "         ...,\n",
       "         [0.6275, 0.6039, 0.6039,  ..., 0.6196, 0.6078, 0.6510],\n",
       "         [0.7882, 0.7686, 0.7725,  ..., 0.7686, 0.7725, 0.7882],\n",
       "         [0.9020, 0.8980, 0.8980,  ..., 0.9098, 0.9098, 0.9176]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample_500[9][4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "6\n",
      "4\n",
      "6\n",
      "3\n",
      "6\n",
      "1\n",
      "2\n",
      "8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for img, label in random_split(training_dataset, [49990, 10])[1]:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we can also compute the entropy value for each class subset and store them into a dictionary, but first we should get the feature space of the subset of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_feature = {category: [] for category in subsample_500}\n",
    "\n",
    "for category in subsample_500:\n",
    "    for img, label in subsample_500[category]:\n",
    "        output = model(to_device(img.unsqueeze(0), device))\n",
    "        subset_feature[label].append(features['18'])\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in subset_feature:\n",
    "    subset_feature[label] = torch.cat(subset_feature[label], dim = 0).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 500])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_feature[8].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the subset feature space of each class, we can get the entropy value for each subset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_singular_vector = {label: torch.svd(subset_feature[label])[1] for label in subset_feature}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7540928303945171,\n",
       " 1: 0.7216651008432696,\n",
       " 2: 0.7752456053919772,\n",
       " 3: 0.7556253710915124,\n",
       " 4: 0.7573073136757312,\n",
       " 5: 0.7451214958553789,\n",
       " 6: 0.7313736665594192,\n",
       " 7: 0.7346624985078555,\n",
       " 8: 0.7443108041732241,\n",
       " 9: 0.7325014300376521}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_entropy = {label: BSIE(truncate(subset_singular_vector[label], 3), [0, float('nan')]).item() for label in subset_singular_vector}\n",
    "subset_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy difference between each class of 5000 and its subset of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.002471324569684901,\n",
       " 1: 0.007238824805253314,\n",
       " 2: 0.006143323188323033,\n",
       " 3: 0.0009285523217714542,\n",
       " 4: 0.0021690337743456925,\n",
       " 5: 0.0034146519198596246,\n",
       " 6: 0.01501650097133133,\n",
       " 7: 0.006519295119853056,\n",
       " 8: 0.001369504227391233,\n",
       " 9: 0.0071828748375236005}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_diff = {label: np.abs(entropy[label] - subset_entropy[label])/entropy[label] for label in entropy}\n",
    "entropy_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we can combine all of each class subset of 500 to train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cifar10sub = ConcatDataset([subsample_500[class_name] for class_name in subsample_500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10sub_loader = DataLoader(cifar10sub, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
    "cifar10sub_train_loader = DeviceDataLoader(cifar10sub_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val_size = 5000\n",
    "train_size = len(training_dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(training_dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "model = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.2805, val_loss: 2.1816, val_acc: 0.1515\n",
      "Epoch [1], train_loss: 2.0766, val_loss: 2.0716, val_acc: 0.2220\n",
      "Epoch [2], train_loss: 1.9531, val_loss: 2.0896, val_acc: 0.2434\n",
      "Epoch [3], train_loss: 1.8082, val_loss: 1.7448, val_acc: 0.3138\n",
      "Epoch [4], train_loss: 1.6875, val_loss: 1.7925, val_acc: 0.3212\n",
      "Epoch [5], train_loss: 1.6192, val_loss: 1.6130, val_acc: 0.3861\n",
      "Epoch [6], train_loss: 1.5532, val_loss: 1.5676, val_acc: 0.4141\n",
      "Epoch [7], train_loss: 1.4969, val_loss: 1.6884, val_acc: 0.3926\n",
      "Epoch [8], train_loss: 1.4454, val_loss: 1.4568, val_acc: 0.4623\n",
      "Epoch [9], train_loss: 1.3349, val_loss: 1.3949, val_acc: 0.4858\n",
      "Epoch [10], train_loss: 1.2601, val_loss: 1.4453, val_acc: 0.4815\n",
      "Epoch [11], train_loss: 1.2108, val_loss: 1.4306, val_acc: 0.4905\n",
      "Epoch [12], train_loss: 1.1182, val_loss: 1.4275, val_acc: 0.4999\n",
      "Epoch [13], train_loss: 1.0606, val_loss: 1.4567, val_acc: 0.4928\n",
      "Epoch [14], train_loss: 0.9861, val_loss: 1.3677, val_acc: 0.5375\n"
     ]
    }
   ],
   "source": [
    "history = fit(num_epochs, lr, model, cifar10sub_train_loader, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: (0, 1000), 1:(1000, 2000), 2:(2000, 3000), 3:(3000, 4000), 4:(4000, 5000), 5:(5000, 6000), 6:(6000, 7000), 7:(7000, 8000), 8:(8000, 9000), 9:(9000, 10000)}\n",
    "test_data_by_class = {category: Subset(ImageFolder('./data/cifar10/test/', transform=ToTensor()), range(classes[category][0], classes[category][1])) for category in classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accuracy_by_class = {label: [] for label in test_data_by_class}\n",
    "\n",
    "for label in test_data_by_class:\n",
    "    test_data = training_data_by_class[label]\n",
    "    test_loader = DeviceDataLoader(DataLoader(test_data, batch_size*2), device)\n",
    "    result = evaluate(model, test_loader)\n",
    "    result\n",
    "    test_accuracy_by_class[label] = result['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.603320300579071,\n",
       " 1: 0.7005055546760559,\n",
       " 2: 0.3923138976097107,\n",
       " 3: 0.22620634734630585,\n",
       " 4: 0.5449333786964417,\n",
       " 5: 0.412109375,\n",
       " 6: 0.4820197522640228,\n",
       " 7: 0.5696461796760559,\n",
       " 8: 0.7796300649642944,\n",
       " 9: 0.6216452121734619}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(list(test_accuracy_by_class.items()), columns=['class', 'Test_Accuracy'])\n",
    "df2 = pd.DataFrame(list(entropy_diff.items()), columns=['class', 'Entropy_difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   class  Entropy_difference\n",
       " 0      0            0.002471\n",
       " 1      1            0.007239\n",
       " 2      2            0.006143\n",
       " 3      3            0.000929\n",
       " 4      4            0.002169\n",
       " 5      5            0.003415\n",
       " 6      6            0.015017\n",
       " 7      7            0.006519\n",
       " 8      8            0.001370\n",
       " 9      9            0.007183,\n",
       "    class  Test_Accuracy\n",
       " 0      0       0.603320\n",
       " 1      1       0.700506\n",
       " 2      2       0.392314\n",
       " 3      3       0.226206\n",
       " 4      4       0.544933\n",
       " 5      5       0.412109\n",
       " 6      6       0.482020\n",
       " 7      7       0.569646\n",
       " 8      8       0.779630\n",
       " 9      9       0.621645)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2, df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Entropy_difference', ylabel='Test_Accuracy'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "merged_df = pd.merge(df1, df2, on='class')\n",
    "sns.scatterplot(data=merged_df, x='Entropy_difference', y='Test_Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After running the code a few more times, there are some correlation that whenever the entropy difference between a subsample and a class grows large, then test set accuracy on that class decreases if we use the subsample as part of our training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
