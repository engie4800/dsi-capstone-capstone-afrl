{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our training dataset and pretrained CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/cifar10'\n",
    "training_dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 5000\n",
    "train_size = len(training_dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(training_dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)) # this is the final CNN layer before classification prediction \n",
    "                                # (the one produce M features in Mr.Diggans' letter) and in this case M = 10\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('cifar10-cnn.pth')) # load our pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cifar10CnnModel(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feature space from intermediate layer just before the final classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x29313cf49d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network[18].register_forward_hook(get_features('18'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two example images we have from our trainset of 45000 images. We are experimenting to see if we can successfully extract their feature space of the intermediate layer just before the final classification layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_ds[40015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(to_device(img.unsqueeze(0), device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['18'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-33.3030,  16.9400, -16.4483, -17.0487,  -9.4894, -12.7982, -12.6007,\n",
       "          -5.9499,  -6.1150, -14.3367,  -5.3236, -16.0510,  -9.7710, -17.0389,\n",
       "         -12.0723, -21.7537,  20.9728,  -7.9361,  -9.0360, -15.7313,  17.9564,\n",
       "         -18.9220,  27.0687, -10.2720, -14.4215,  25.5399,  -4.8923,   5.2339,\n",
       "         -14.4592, -17.0238, -12.9081, -17.8656,  20.1692, -12.5104,  10.0218,\n",
       "          -8.7546,   3.0353, -10.8188,  -5.0133, -12.7305,  -7.6203,  -6.6084,\n",
       "          -8.5120,  -8.0371,   6.6164,  -8.3862,  -7.2923, -15.4102,  15.4927,\n",
       "         -13.0282,  -8.5688,   3.2616,   3.6510,  -5.7958,   1.8589, -15.4129,\n",
       "         -15.0838, -13.0273,  -4.4431, -11.6662,  -8.0818,  -9.0962, -10.6501,\n",
       "         -13.3178]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = train_ds[40010]\n",
    "output = model(to_device(img.unsqueeze(0), device))\n",
    "features['18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be successful, so now we can try to extract the feature space of every training image to form a vector. These vectors will then form a huge matrix of the dimension 64 * 45000 to representing the whole training dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset matrix with feature space (number of rows) of 64 for 45000 image vectors (number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_matrix = []\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    img, label = train_ds[i]\n",
    "    output = model(to_device(img.unsqueeze(0), device))\n",
    "    dataset_matrix.append(features['18'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the matrix (of the dimension 64 * 45000) to represent the whole training dataset of 45000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.2748, -33.5281, -21.3351,  ...,  -5.6529, -23.5119, -15.3105],\n",
       "        [-13.1279,  -3.9095, -18.1370,  ...,  -1.2432,   2.8258,   7.8583],\n",
       "        [-19.0318, -25.4091, -21.0193,  ..., -16.0697, -12.8252,  -8.1891],\n",
       "        ...,\n",
       "        [-18.8240, -19.4958, -14.0463,  ..., -16.0955, -12.8271, -11.0381],\n",
       "        [-19.0703, -28.2956, -24.7656,  ..., -17.7981, -16.3135, -20.5031],\n",
       "        [ -8.6824, -20.5190, -11.5298,  ...,  -7.2010,  -8.4604,  -4.0791]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_matrix = torch.cat(dataset_matrix, dim = 0).T \n",
    "dataset_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 45000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the feature matrix of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_matrix).to_csv(f\"C:/Columbia_University/Research/Capstone/Data/trainset_feature.csv\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = training_dataset[45555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4157, 0.4196, 0.4353,  ..., 0.3882, 0.3843, 0.3804],\n",
       "         [0.4510, 0.4549, 0.4706,  ..., 0.4000, 0.3961, 0.3922],\n",
       "         [0.4706, 0.4706, 0.4824,  ..., 0.4078, 0.4039, 0.4000],\n",
       "         ...,\n",
       "         [0.8039, 0.8314, 0.8667,  ..., 0.7725, 0.8471, 0.8471],\n",
       "         [0.8353, 0.8157, 0.8235,  ..., 0.8157, 0.8196, 0.8235],\n",
       "         [0.8118, 0.8039, 0.8196,  ..., 0.8000, 0.8235, 0.8314]],\n",
       "\n",
       "        [[0.5922, 0.5961, 0.6118,  ..., 0.5529, 0.5490, 0.5451],\n",
       "         [0.6118, 0.6157, 0.6314,  ..., 0.5686, 0.5647, 0.5569],\n",
       "         [0.6196, 0.6196, 0.6314,  ..., 0.5765, 0.5686, 0.5647],\n",
       "         ...,\n",
       "         [0.8471, 0.8745, 0.9098,  ..., 0.8039, 0.8824, 0.8824],\n",
       "         [0.8784, 0.8588, 0.8667,  ..., 0.8510, 0.8588, 0.8627],\n",
       "         [0.8549, 0.8471, 0.8588,  ..., 0.8392, 0.8667, 0.8824]],\n",
       "\n",
       "        [[0.7529, 0.7529, 0.7725,  ..., 0.7294, 0.7333, 0.7373],\n",
       "         [0.7804, 0.7843, 0.8000,  ..., 0.7451, 0.7490, 0.7529],\n",
       "         [0.7922, 0.7882, 0.8000,  ..., 0.7529, 0.7569, 0.7608],\n",
       "         ...,\n",
       "         [0.8314, 0.8588, 0.8941,  ..., 0.7765, 0.8627, 0.8745],\n",
       "         [0.8627, 0.8431, 0.8510,  ..., 0.8471, 0.8627, 0.8667],\n",
       "         [0.8392, 0.8314, 0.8431,  ..., 0.8471, 0.8706, 0.8863]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This following function helps us to randomly select certain number of images from each class in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(trainset, num_images_per_class = 500):\n",
    "    \n",
    "    selected_indices_per_class = {i: [] for i in range(10)}\n",
    "\n",
    "\n",
    "    for index, (image, label) in enumerate(trainset):\n",
    "        selected_indices_per_class[label].append(index)\n",
    "\n",
    "    for class_label in selected_indices_per_class:\n",
    "        np.random.shuffle(selected_indices_per_class[class_label])\n",
    "\n",
    "\n",
    "    selected_indices = []\n",
    "    for class_label in selected_indices_per_class:\n",
    "        selected_indices.extend(selected_indices_per_class[class_label][:num_images_per_class])\n",
    "\n",
    "    \n",
    "    selected_images = [trainset[index][0] for index in selected_indices]\n",
    "\n",
    "    return selected_indices, selected_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature space of 200 randomly selected subsamples (equally selected from each class) and save them locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to execute the cell below every time if we already have enough subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    selected_indices, selected_images = subsample(training_dataset)\n",
    "    print(len(selected_images))\n",
    "    intermediate_features = []\n",
    "    for img in selected_images:\n",
    "        output = model(to_device(img.unsqueeze(0), device))\n",
    "        intermediate_features.append(features['18'])\n",
    "\n",
    "    intermediate_features = torch.cat(intermediate_features, dim = 0).T \n",
    "    pd.DataFrame(intermediate_features).to_csv(f\"C:/Columbia_University/Research/Capstone/Data/intermediate_feature{i}.csv\", index = False, header = False)\n",
    "    pd.DataFrame({'imgIndex': selected_indices}).to_csv(f\"C:/Columbia_University/Research/Capstone/Data/index_{i}.csv\", index = False)\n",
    "    # We need to store the indices of images in order for us to retrieve those images from the training set. These images will be used later to \n",
    "    # retrain the CNN and confirm we get the these exact images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the matrices of feature space back and apply SVD and BSIE on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BSI_Entropy import BSIE\n",
    "Entropy = {}\n",
    "for i in range(200):\n",
    "    feature_matrix = pd.read_csv(f\"C:/Columbia_University/Research/Capstone/Data/intermediate_feature{i}.csv\", header = None)\n",
    "    U, S, V = torch.svd(torch.tensor(feature_matrix.values))\n",
    "    key = f\"subsample_{i}\"\n",
    "    Entropy[key] = BSIE(S).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### That is, we save all the entropy values of all the subsamples in one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample_0': 0.06525616774755605,\n",
       " 'subsample_1': 0.06601550184495408,\n",
       " 'subsample_2': 0.06583738878277523,\n",
       " 'subsample_3': 0.06588003418194033,\n",
       " 'subsample_4': 0.06621561346191263,\n",
       " 'subsample_5': 0.06609114640029734,\n",
       " 'subsample_6': 0.06524106166743482,\n",
       " 'subsample_7': 0.0662361586657545,\n",
       " 'subsample_8': 0.06550520598930898,\n",
       " 'subsample_9': 0.06558301145228662,\n",
       " 'subsample_10': 0.06376776601196743,\n",
       " 'subsample_11': 0.06543490442498667,\n",
       " 'subsample_12': 0.06540695499814186,\n",
       " 'subsample_13': 0.06390118766740938,\n",
       " 'subsample_14': 0.0660047469238616,\n",
       " 'subsample_15': 0.06555204454416985,\n",
       " 'subsample_16': 0.06479973599034716,\n",
       " 'subsample_17': 0.06548055490708249,\n",
       " 'subsample_18': 0.06493505076885586,\n",
       " 'subsample_19': 0.06514160543903769,\n",
       " 'subsample_20': 0.06589275088330471,\n",
       " 'subsample_21': 0.06387354818425373,\n",
       " 'subsample_22': 0.06672664647062265,\n",
       " 'subsample_23': 0.0647495647314984,\n",
       " 'subsample_24': 0.06601403724663168,\n",
       " 'subsample_25': 0.06550203445559888,\n",
       " 'subsample_26': 0.06757319446217247,\n",
       " 'subsample_27': 0.06627192968856066,\n",
       " 'subsample_28': 0.06629941268714223,\n",
       " 'subsample_29': 0.06760879063708203,\n",
       " 'subsample_30': 0.0660866600115746,\n",
       " 'subsample_31': 0.06622558751068597,\n",
       " 'subsample_32': 0.06527495449872267,\n",
       " 'subsample_33': 0.06701861563072353,\n",
       " 'subsample_34': 0.06464207488362694,\n",
       " 'subsample_35': 0.06437478777273564,\n",
       " 'subsample_36': 0.06524862800269715,\n",
       " 'subsample_37': 0.06626846094634664,\n",
       " 'subsample_38': 0.0651441191875698,\n",
       " 'subsample_39': 0.06542867683879905,\n",
       " 'subsample_40': 0.06621346466777689,\n",
       " 'subsample_41': 0.0657668756308013,\n",
       " 'subsample_42': 0.06649243853386444,\n",
       " 'subsample_43': 0.06598398385091653,\n",
       " 'subsample_44': 0.06625060864496535,\n",
       " 'subsample_45': 0.06564963729620676,\n",
       " 'subsample_46': 0.06511235786735337,\n",
       " 'subsample_47': 0.06601436313048714,\n",
       " 'subsample_48': 0.06504820870047645,\n",
       " 'subsample_49': 0.06596206476864797,\n",
       " 'subsample_50': 0.06483566290831222,\n",
       " 'subsample_51': 0.06596329100675136,\n",
       " 'subsample_52': 0.06601696444003613,\n",
       " 'subsample_53': 0.06677070296085241,\n",
       " 'subsample_54': 0.06621908101759788,\n",
       " 'subsample_55': 0.06336387568863122,\n",
       " 'subsample_56': 0.0659121809018266,\n",
       " 'subsample_57': 0.06613312236186986,\n",
       " 'subsample_58': 0.06433482157864989,\n",
       " 'subsample_59': 0.06493668579214151,\n",
       " 'subsample_60': 0.06542066335394014,\n",
       " 'subsample_61': 0.06657094874583136,\n",
       " 'subsample_62': 0.06570187104340441,\n",
       " 'subsample_63': 0.06517291835265959,\n",
       " 'subsample_64': 0.06563075817926223,\n",
       " 'subsample_65': 0.06592441029624596,\n",
       " 'subsample_66': 0.06698339941755993,\n",
       " 'subsample_67': 0.06510753163436656,\n",
       " 'subsample_68': 0.06621479032425515,\n",
       " 'subsample_69': 0.06521925630342662,\n",
       " 'subsample_70': 0.06544496359221574,\n",
       " 'subsample_71': 0.06703207321097615,\n",
       " 'subsample_72': 0.06557940648864802,\n",
       " 'subsample_73': 0.06606371205744321,\n",
       " 'subsample_74': 0.0653404692989733,\n",
       " 'subsample_75': 0.06565689979322631,\n",
       " 'subsample_76': 0.06618091085614719,\n",
       " 'subsample_77': 0.06549583817041282,\n",
       " 'subsample_78': 0.06450755079346726,\n",
       " 'subsample_79': 0.0665996402801019,\n",
       " 'subsample_80': 0.06731317420654592,\n",
       " 'subsample_81': 0.06690941593426747,\n",
       " 'subsample_82': 0.06604251274426687,\n",
       " 'subsample_83': 0.06595206214830429,\n",
       " 'subsample_84': 0.0649248135923931,\n",
       " 'subsample_85': 0.06655467342966903,\n",
       " 'subsample_86': 0.06601420993436713,\n",
       " 'subsample_87': 0.0661562919976636,\n",
       " 'subsample_88': 0.06621539388146502,\n",
       " 'subsample_89': 0.06485060479170368,\n",
       " 'subsample_90': 0.06497059233153857,\n",
       " 'subsample_91': 0.06554008398720113,\n",
       " 'subsample_92': 0.06660626645374801,\n",
       " 'subsample_93': 0.06476897810191484,\n",
       " 'subsample_94': 0.06531769362061357,\n",
       " 'subsample_95': 0.06450233246585835,\n",
       " 'subsample_96': 0.06490820152442534,\n",
       " 'subsample_97': 0.06636420282736721,\n",
       " 'subsample_98': 0.06523977867768604,\n",
       " 'subsample_99': 0.06596296187224437,\n",
       " 'subsample_100': 0.06454024732530139,\n",
       " 'subsample_101': 0.06706545005970987,\n",
       " 'subsample_102': 0.06647502959890161,\n",
       " 'subsample_103': 0.06608677434036558,\n",
       " 'subsample_104': 0.06569078208679935,\n",
       " 'subsample_105': 0.06475940664278002,\n",
       " 'subsample_106': 0.06589951491839874,\n",
       " 'subsample_107': 0.06460251268048578,\n",
       " 'subsample_108': 0.0653531680964976,\n",
       " 'subsample_109': 0.06521522197073581,\n",
       " 'subsample_110': 0.06562881867572268,\n",
       " 'subsample_111': 0.06580896613282061,\n",
       " 'subsample_112': 0.06434932342231658,\n",
       " 'subsample_113': 0.06561260627865939,\n",
       " 'subsample_114': 0.06676861458921612,\n",
       " 'subsample_115': 0.06616053469658167,\n",
       " 'subsample_116': 0.0660122998847229,\n",
       " 'subsample_117': 0.06488986251868545,\n",
       " 'subsample_118': 0.06729998741271415,\n",
       " 'subsample_119': 0.06547282831483248,\n",
       " 'subsample_120': 0.06678528630986258,\n",
       " 'subsample_121': 0.0662991159430063,\n",
       " 'subsample_122': 0.06556451542739539,\n",
       " 'subsample_123': 0.0651372470443281,\n",
       " 'subsample_124': 0.06593433685261008,\n",
       " 'subsample_125': 0.066264788786348,\n",
       " 'subsample_126': 0.06709844275657029,\n",
       " 'subsample_127': 0.06672461290352205,\n",
       " 'subsample_128': 0.06588046019101901,\n",
       " 'subsample_129': 0.064879142719388,\n",
       " 'subsample_130': 0.06580534004643246,\n",
       " 'subsample_131': 0.06507499398293481,\n",
       " 'subsample_132': 0.06503283984794683,\n",
       " 'subsample_133': 0.06516293971371101,\n",
       " 'subsample_134': 0.06492844715665447,\n",
       " 'subsample_135': 0.06430511220802304,\n",
       " 'subsample_136': 0.06495803922135046,\n",
       " 'subsample_137': 0.06541434042077021,\n",
       " 'subsample_138': 0.06532310709161626,\n",
       " 'subsample_139': 0.06648653367383528,\n",
       " 'subsample_140': 0.06591227146083722,\n",
       " 'subsample_141': 0.06447637429183695,\n",
       " 'subsample_142': 0.0670514055474194,\n",
       " 'subsample_143': 0.06538520211277277,\n",
       " 'subsample_144': 0.06519635821411529,\n",
       " 'subsample_145': 0.06421896751794587,\n",
       " 'subsample_146': 0.0656652463714874,\n",
       " 'subsample_147': 0.06540598999053726,\n",
       " 'subsample_148': 0.0657711154971381,\n",
       " 'subsample_149': 0.06463440346318305,\n",
       " 'subsample_150': 0.06444528723943,\n",
       " 'subsample_151': 0.0659935649216693,\n",
       " 'subsample_152': 0.06602958025987016,\n",
       " 'subsample_153': 0.06642116811771448,\n",
       " 'subsample_154': 0.06646260996643683,\n",
       " 'subsample_155': 0.06546860100358076,\n",
       " 'subsample_156': 0.06606611713747956,\n",
       " 'subsample_157': 0.06470713514069137,\n",
       " 'subsample_158': 0.06671761889660188,\n",
       " 'subsample_159': 0.0660366725603655,\n",
       " 'subsample_160': 0.06545900312462072,\n",
       " 'subsample_161': 0.06588628977953659,\n",
       " 'subsample_162': 0.0643529577862677,\n",
       " 'subsample_163': 0.06507839004410931,\n",
       " 'subsample_164': 0.06592086165166633,\n",
       " 'subsample_165': 0.06444601135447225,\n",
       " 'subsample_166': 0.06591966964732521,\n",
       " 'subsample_167': 0.06644975913398266,\n",
       " 'subsample_168': 0.06549687752097899,\n",
       " 'subsample_169': 0.0658209777742974,\n",
       " 'subsample_170': 0.06526909686463112,\n",
       " 'subsample_171': 0.06416353297054778,\n",
       " 'subsample_172': 0.06588592416301986,\n",
       " 'subsample_173': 0.06620672730027377,\n",
       " 'subsample_174': 0.06546523605470977,\n",
       " 'subsample_175': 0.06449287670505266,\n",
       " 'subsample_176': 0.0653462521105811,\n",
       " 'subsample_177': 0.06408021864168945,\n",
       " 'subsample_178': 0.066678310926651,\n",
       " 'subsample_179': 0.0655206957001715,\n",
       " 'subsample_180': 0.06502619344488425,\n",
       " 'subsample_181': 0.06537211059138837,\n",
       " 'subsample_182': 0.0661649219390692,\n",
       " 'subsample_183': 0.06613736991188135,\n",
       " 'subsample_184': 0.06465013702518585,\n",
       " 'subsample_185': 0.06680394830839198,\n",
       " 'subsample_186': 0.06540776699098971,\n",
       " 'subsample_187': 0.0643103588100421,\n",
       " 'subsample_188': 0.06628801319983035,\n",
       " 'subsample_189': 0.06498120729189127,\n",
       " 'subsample_190': 0.06417412629085684,\n",
       " 'subsample_191': 0.06631303279679546,\n",
       " 'subsample_192': 0.06633030314543753,\n",
       " 'subsample_193': 0.06550048489618321,\n",
       " 'subsample_194': 0.06445948718338468,\n",
       " 'subsample_195': 0.06510441621850249,\n",
       " 'subsample_196': 0.06633982262329419,\n",
       " 'subsample_197': 0.0650190136520894,\n",
       " 'subsample_198': 0.06665925813337692,\n",
       " 'subsample_199': 0.06581157885429467}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attain the entropy value of the whole CIFAR-10 training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0654302882809098\n"
     ]
    }
   ],
   "source": [
    "trainset_matrix = pd.read_csv(f\"C:/Columbia_University/Research/Capstone/Data/trainset_feature.csv\", header = None)\n",
    "U, S, V = torch.svd(torch.tensor(trainset_matrix.values))\n",
    "entropy_value = BSIE(S).item()\n",
    "print(entropy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the subsample entropy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.histplot(list(Entropy.values()))\n",
    "ax.axvline(x = entropy_value, color = 'red', label = \"CIFAR-10 Training Set Entropy value\")\n",
    "plt.xlabel(\"Entropy Values\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
