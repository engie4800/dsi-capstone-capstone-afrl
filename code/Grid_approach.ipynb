{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Approach to enhance or eliminate bias in the training subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split, DataLoader, SubsetRandomSampler, Subset, ConcatDataset\n",
    "import os\n",
    "from Truncate import truncate\n",
    "from BSI_Entropy import BSIE\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Like what we did in the original file, we set up all the classes/functions properly and prepare our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/cifar10'\n",
    "training_dataset = ImageFolder(data_dir+'/train', transform=ToTensor())\n",
    "test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now create a CNN model object from the class above and assign it trained parameters we stored. This CNN model will be our embedding function to help us extract feature space of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.load_state_dict(torch.load('cifar10-cnn.pth')) # load our pretrained model parameters and assign to the new model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.9372484087944031, 'val_acc': 0.7689453363418579}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size=128\n",
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "evaluate(modelCNN, test_loader) # this will make sure it is the same CNN model we trained initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This following function will help us to attain intermediate features of images before the final classification layer of the CNN model which is our embedding function in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x26b17e89fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.network[18].register_forward_hook(get_features('18'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following code, we attain the intermediate feature space for each class in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_feature_data = {label: [] for label in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in training_dataset:\n",
    "    if label in class_feature_data:\n",
    "        output = modelCNN(to_device(image.unsqueeze(0), device))\n",
    "        class_feature_data[label].append(features['18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in class_feature_data:\n",
    "    class_feature_data[label] = torch.cat(class_feature_data[label], dim = 0).T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We then apply SVD to the feature space of each training class to attain singular vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([8305.8789, 1585.3182, 1073.5406,  963.0099,  828.8642,  769.5621,\n",
      "         616.4266,  580.3412,  516.1616,  488.6954,  402.4275,  341.2084,\n",
      "         275.5750,  266.5753,  257.8827,  231.7271,  198.1219,  183.4829,\n",
      "         180.9243,  177.5941,  156.9813,  144.6703,  141.9810,  132.6933,\n",
      "         126.5861,  122.6644,  117.9465,  113.8699,  110.0822,  106.3363,\n",
      "         103.7124,   93.6782,   91.7617,   89.5256,   84.1956,   82.4179,\n",
      "          80.3859,   78.2227,   75.4202,   72.8308,   71.0707,   69.9203,\n",
      "          65.3629,   64.8393,   62.6921,   60.4378,   58.1620,   56.9152,\n",
      "          54.7706,   54.1711,   52.7214,   51.6815,   50.4430,   49.4335,\n",
      "          47.0124,   45.7740,   43.8802,   43.6093,   42.2480,   40.5315,\n",
      "          39.1501,   37.6497,   36.0807,   34.7374]), 1: tensor([12095.2061,  1403.4337,  1103.4655,   963.6672,   940.5552,   853.0527,\n",
      "          744.2352,   670.6514,   567.7023,   496.4117,   435.3126,   407.3419,\n",
      "          350.2057,   277.2058,   250.3987,   228.0301,   213.1301,   201.9636,\n",
      "          183.1754,   177.0799,   173.2148,   157.2500,   154.9059,   141.8093,\n",
      "          136.7393,   130.5036,   123.9009,   111.7951,   109.5792,   104.0679,\n",
      "          102.0208,    98.5726,    95.6230,    92.5422,    91.4762,    87.7978,\n",
      "           85.6393,    80.0061,    75.8346,    74.1482,    72.9685,    70.6738,\n",
      "           68.0645,    67.4477,    63.2545,    62.6573,    58.7270,    57.1137,\n",
      "           55.9109,    54.9257,    53.4217,    51.6753,    50.8225,    49.1771,\n",
      "           46.7251,    46.5821,    44.0645,    43.7546,    42.5789,    39.9152,\n",
      "           38.9562,    36.2970,    35.4175,    31.8563]), 2: tensor([6268.6768,  996.6603,  861.0262,  768.5312,  739.1451,  694.6696,\n",
      "         600.1634,  527.4907,  457.0351,  414.4943,  364.2594,  286.4704,\n",
      "         252.5366,  232.2138,  215.6590,  201.4022,  193.2244,  170.9879,\n",
      "         168.6637,  152.4425,  139.6423,  135.0280,  128.0127,  120.3685,\n",
      "         113.6636,  107.9870,  103.6013,  101.6421,   95.3172,   93.9559,\n",
      "          88.8051,   85.6524,   82.2905,   80.0169,   76.2634,   74.8454,\n",
      "          73.6839,   72.4649,   67.7688,   66.6625,   64.4223,   61.4686,\n",
      "          60.3447,   58.9813,   57.0994,   55.9028,   54.6610,   53.0541,\n",
      "          49.8968,   48.1705,   46.9777,   46.7670,   45.8985,   43.3955,\n",
      "          42.3475,   41.3480,   40.6217,   39.2341,   37.1279,   36.3377,\n",
      "          35.2792,   34.6068,   33.2892,   31.7638]), 3: tensor([6473.5073,  841.2786,  778.4031,  699.7131,  636.0625,  580.7430,\n",
      "         545.3973,  478.3984,  441.1371,  360.8887,  271.3305,  227.0016,\n",
      "         215.7848,  210.0773,  184.9863,  174.0165,  161.9877,  152.2511,\n",
      "         149.6021,  136.9679,  129.8981,  125.9189,  120.4214,  109.9679,\n",
      "         104.4016,  103.5947,   97.8948,   96.5096,   85.8014,   85.5063,\n",
      "          82.8391,   81.4511,   78.8962,   73.4542,   70.5081,   69.6882,\n",
      "          66.7658,   64.2578,   64.0274,   62.4119,   60.9270,   60.1115,\n",
      "          58.6312,   55.4811,   53.9831,   52.1251,   51.3327,   50.8141,\n",
      "          47.4976,   47.1265,   45.3364,   43.8578,   43.6088,   43.3220,\n",
      "          41.2991,   39.3377,   38.7576,   37.6581,   36.6142,   35.8798,\n",
      "          34.7457,   33.8410,   33.0287,   31.0110]), 4: tensor([7251.3999, 1030.9064,  909.2327,  707.2688,  637.4417,  578.5201,\n",
      "         508.1140,  466.0776,  446.0780,  397.8710,  320.4633,  269.8743,\n",
      "         264.0945,  229.7420,  226.3317,  194.3986,  188.6454,  166.8791,\n",
      "         154.9358,  152.1550,  140.9037,  124.5961,  121.1879,  118.2018,\n",
      "         111.4068,   99.5754,   99.0052,   96.0552,   93.1947,   88.4500,\n",
      "          83.6878,   81.2279,   77.7753,   77.0465,   75.4441,   72.9830,\n",
      "          69.8479,   68.6878,   66.5030,   65.3280,   62.9284,   60.9607,\n",
      "          57.7178,   55.7312,   55.0482,   51.6707,   50.9145,   48.9035,\n",
      "          48.0556,   47.2219,   46.6511,   44.7408,   42.6484,   41.6211,\n",
      "          40.9727,   39.4311,   38.1823,   37.1305,   35.6565,   34.5898,\n",
      "          33.4765,   32.3365,   31.2384,   30.4821]), 5: tensor([6615.5000,  935.8938,  749.0901,  681.4392,  659.0427,  591.3130,\n",
      "         568.2397,  471.9457,  434.5263,  397.9953,  320.4688,  229.8735,\n",
      "         207.7418,  199.3160,  181.2436,  179.3377,  164.5392,  158.5162,\n",
      "         144.6888,  135.9444,  125.6181,  122.4719,  114.7564,  108.4020,\n",
      "          98.2476,   96.3082,   94.3113,   87.3923,   86.2062,   83.2604,\n",
      "          80.6292,   78.4679,   76.5878,   72.7462,   71.2571,   69.3191,\n",
      "          67.5854,   64.7256,   61.7663,   59.2221,   57.9550,   57.0776,\n",
      "          55.5528,   53.5544,   52.0715,   51.1490,   49.4429,   48.2574,\n",
      "          47.1759,   45.2133,   44.9031,   42.5675,   41.7146,   40.3162,\n",
      "          39.2917,   38.8730,   38.4060,   36.5054,   34.4352,   33.8537,\n",
      "          32.6440,   32.3013,   31.1622,   29.4608]), 6: tensor([7264.2148, 1025.9622,  821.9323,  766.3347,  668.6025,  559.7619,\n",
      "         490.6772,  444.9507,  408.4172,  347.1433,  305.5564,  228.6729,\n",
      "         222.3128,  206.2740,  182.9453,  170.8316,  157.5761,  153.2848,\n",
      "         131.4534,  127.3159,  126.6383,  111.9518,  103.7711,  100.5675,\n",
      "          94.8066,   89.5136,   87.9663,   82.4812,   80.3807,   75.5823,\n",
      "          75.0115,   72.9168,   70.3978,   67.0040,   66.0912,   62.3881,\n",
      "          61.8958,   60.0585,   58.2270,   55.7178,   53.8061,   52.5783,\n",
      "          49.9245,   48.5092,   48.1664,   46.0542,   45.5001,   43.5996,\n",
      "          41.0822,   40.6236,   39.1936,   38.3900,   36.9234,   36.2563,\n",
      "          35.1064,   34.7974,   33.9532,   31.8801,   31.6158,   30.7459,\n",
      "          29.4570,   27.8902,   26.1976,   25.0586]), 7: tensor([9169.8828, 1493.6215, 1051.6710,  827.7892,  704.5131,  648.9139,\n",
      "         604.9308,  552.1254,  529.1401,  405.0483,  379.9564,  326.4211,\n",
      "         275.0515,  241.4179,  225.5518,  223.2711,  190.9410,  185.7654,\n",
      "         180.1633,  159.7839,  149.9531,  140.9736,  129.3146,  123.8428,\n",
      "         118.9251,  117.3516,  110.0507,  101.6755,  100.0895,   94.0084,\n",
      "          89.3221,   87.6476,   83.3122,   83.0177,   79.0291,   77.2495,\n",
      "          76.2627,   75.2523,   72.6804,   71.6437,   68.6938,   65.9932,\n",
      "          62.9971,   61.2922,   59.8471,   58.7325,   57.2511,   55.7114,\n",
      "          54.3630,   52.0995,   49.9387,   48.9176,   47.1538,   46.9003,\n",
      "          44.8975,   43.6781,   41.0143,   40.4583,   40.1294,   39.0184,\n",
      "          37.9315,   34.9119,   34.0490,   32.1201]), 8: tensor([9594.4805, 1124.0610, 1052.3362, 1004.5804,  869.7374,  671.5615,\n",
      "         621.2519,  523.7562,  475.3379,  443.6017,  403.6830,  317.1657,\n",
      "         286.7527,  249.1192,  224.5342,  216.5424,  185.4713,  184.0177,\n",
      "         174.7412,  168.6416,  144.3980,  139.9327,  130.4194,  128.2969,\n",
      "         119.3904,  116.7353,  110.4874,  104.6957,  102.1369,  100.8237,\n",
      "          93.3471,   87.9545,   87.0240,   81.4927,   76.3620,   74.8747,\n",
      "          72.8455,   72.1757,   69.9687,   67.7925,   63.9059,   63.6517,\n",
      "          61.2246,   57.5016,   57.1501,   54.6067,   54.3347,   52.6592,\n",
      "          51.7210,   50.4249,   48.6767,   46.9900,   45.1177,   43.7656,\n",
      "          41.3862,   39.9052,   39.1436,   38.6954,   36.9722,   35.7993,\n",
      "          34.9810,   33.8001,   32.5454,   30.0804]), 9: tensor([10423.7295,  1075.6924,  1005.2132,   868.2537,   845.2270,   768.0860,\n",
      "          653.5539,   533.2513,   448.2159,   377.9797,   349.4870,   321.1977,\n",
      "          258.0855,   248.5007,   222.5318,   209.2140,   185.0927,   176.9587,\n",
      "          170.2491,   150.6953,   141.7572,   133.9512,   126.0533,   120.6464,\n",
      "          114.3579,   106.9467,   101.3006,   100.5582,    96.6379,    94.3995,\n",
      "           91.8883,    90.4309,    87.2283,    83.8863,    80.6998,    77.5528,\n",
      "           76.5992,    75.3139,    74.1166,    70.6445,    68.6330,    63.5477,\n",
      "           61.1679,    60.1646,    56.9889,    55.9821,    54.9749,    54.2968,\n",
      "           52.9092,    51.9405,    50.1667,    48.8075,    46.8235,    46.0916,\n",
      "           45.4860,    44.6703,    41.4841,    40.2458,    37.5727,    36.9767,\n",
      "           36.2387,    35.1060,    33.5660,    30.2874])}\n"
     ]
    }
   ],
   "source": [
    "singular_vector = {label: torch.svd(class_feature_data[label])[1] for label in class_feature_data}\n",
    "print(singular_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note here, we don't truncate singular vector anymore to get BSIE value because the singular vector of each training class feature space contains values apparently greater than 0 almost at each entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.06290901692797712, 1: 0.06283993894178475, 2: 0.06495570296841247, 3: 0.06314928805309994, 4: 0.06271682050760996, 5: 0.06349178769941977, 6: 0.06271585297092763, 7: 0.062195439924848994, 8: 0.06215472324653093, 9: 0.0626122551072612}\n"
     ]
    }
   ],
   "source": [
    "entropy = {label: BSIE(singular_vector[label]).item() for label in singular_vector}\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to select representative and unrepresentative subsamples for each class and combine them together to train CNN submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: (0, 5000), 1:(5000, 10000), 2:(10000, 15000), 3:(15000, 20000), 4:(20000, 25000), 5:(25000, 30000), 6:(30000, 35000), 7:(35000, 40000), 8:(40000, 45000), 9:(45000, 50000)}\n",
    "training_data_by_class = {category: Subset(ImageFolder('./data/cifar10/train/', transform=ToTensor()), range(classes[category][0], classes[category][1])) for category in classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_subsample_per_class(classified_training_data, num = 15, n_sample = 500):\n",
    "    subsample = {j: [random_split(classified_training_data[j], [n_sample, len(classified_training_data[j]) - n_sample])[0] for round in range(num)] for j in classified_training_data}\n",
    "    return subsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6157, 0.5843, 0.5804,  ..., 0.5765, 0.5608, 0.5529],\n",
       "         [0.7490, 0.6157, 0.5686,  ..., 0.5804, 0.5686, 0.5608],\n",
       "         [0.9333, 0.8157, 0.6863,  ..., 0.5961, 0.5804, 0.5725],\n",
       "         ...,\n",
       "         [0.9686, 0.9647, 0.9843,  ..., 0.9608, 0.9490, 0.9294],\n",
       "         [0.7490, 0.7373, 0.7529,  ..., 0.7804, 0.7373, 0.7216],\n",
       "         [0.3412, 0.3373, 0.3255,  ..., 0.2784, 0.3412, 0.3216]],\n",
       "\n",
       "        [[0.5804, 0.5490, 0.5490,  ..., 0.5529, 0.5373, 0.5294],\n",
       "         [0.7255, 0.5922, 0.5490,  ..., 0.5569, 0.5451, 0.5373],\n",
       "         [0.9255, 0.8039, 0.6706,  ..., 0.5725, 0.5569, 0.5490],\n",
       "         ...,\n",
       "         [0.9529, 0.9529, 0.9686,  ..., 0.9608, 0.9490, 0.9294],\n",
       "         [0.7451, 0.7333, 0.7451,  ..., 0.7804, 0.7373, 0.7216],\n",
       "         [0.3412, 0.3333, 0.3216,  ..., 0.2784, 0.3412, 0.3216]],\n",
       "\n",
       "        [[0.6235, 0.6000, 0.6039,  ..., 0.5961, 0.5843, 0.5804],\n",
       "         [0.7608, 0.6353, 0.5961,  ..., 0.6000, 0.5922, 0.5922],\n",
       "         [0.9412, 0.8275, 0.7020,  ..., 0.6118, 0.6039, 0.6000],\n",
       "         ...,\n",
       "         [0.9608, 0.9569, 0.9725,  ..., 0.9608, 0.9490, 0.9294],\n",
       "         [0.7490, 0.7373, 0.7490,  ..., 0.7804, 0.7373, 0.7216],\n",
       "         [0.3412, 0.3333, 0.3216,  ..., 0.2784, 0.3412, 0.3216]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsamples_per_class = num_subsample_per_class(training_data_by_class)\n",
    "subsamples_per_class[2][13][499][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_space(dictionary):\n",
    "    feature_space_dict = {label: [[] for _ in range(len(dictionary[label]))] for label in dictionary}\n",
    "    for category in dictionary:\n",
    "        for i in range(len(dictionary[category])):\n",
    "            subsample = dictionary[category][i]\n",
    "            for img, lbl in subsample:\n",
    "                if lbl in feature_space_dict:\n",
    "                    output = modelCNN(to_device(img.unsqueeze(0), device))\n",
    "                    feature_space_dict[lbl][i].append(features['18'])\n",
    "\n",
    "            feature_space_dict[category][i] = torch.cat(feature_space_dict[category][i], dim = 0).T\n",
    "\n",
    "    return feature_space_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, we want to confirm we extract a list of feature space for each class 0 to 9 correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space_per_class = feature_space(subsamples_per_class)#[3][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 500])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_space_per_class[3][14].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.1934, -15.1389, -11.6547,  ...,  -9.2845, -10.4516, -11.7140],\n",
       "        [ 10.4669,   5.0653,   2.3566,  ...,   4.2572,   4.4443,  -4.6796],\n",
       "        [-10.3823,  -8.5444,  -5.3777,  ...,  -7.9962, -12.2085,  -9.7331],\n",
       "        ...,\n",
       "        [-15.6902,  -8.4537,  -9.8786,  ..., -12.3004, -12.9953,  -7.4532],\n",
       "        [-13.4329,  -9.8185, -14.7792,  ...,  -7.8993, -18.7558, -17.7395],\n",
       "        [-11.2296,  -6.3066,  -2.5546,  ...,  -6.4803,  -5.3827,  -5.1181]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_space_per_class[3][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to singular vectors\n",
    "def entropy_values_per_class(dict):\n",
    "    return {label: [BSIE(torch.svd(dict[label][i])[1]).item() for i in range(len(dict[label]))] for label in dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_per_class = entropy_values_per_class(feature_space_per_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.06223174332163228,\n",
       "  0.062246221860963735,\n",
       "  0.0628794847375328,\n",
       "  0.0637740895195934,\n",
       "  0.06421946467022688,\n",
       "  0.06290882283548138,\n",
       "  0.06358153449254556,\n",
       "  0.06362399008650366,\n",
       "  0.06233864570751513,\n",
       "  0.06402329799622464,\n",
       "  0.06347326541508969,\n",
       "  0.06055270259029799,\n",
       "  0.062201170161862884,\n",
       "  0.06248273054828024,\n",
       "  0.06271376259744132],\n",
       " 1: [0.062216562258675334,\n",
       "  0.06218319541272188,\n",
       "  0.06268658413888195,\n",
       "  0.06242857923872469,\n",
       "  0.06272799452909261,\n",
       "  0.06164511961109265,\n",
       "  0.062246260649935214,\n",
       "  0.06153709062793011,\n",
       "  0.062375147395955954,\n",
       "  0.06334838670208398,\n",
       "  0.06175129960650061,\n",
       "  0.06217897599428257,\n",
       "  0.06289207694436327,\n",
       "  0.06154366456426197,\n",
       "  0.06249472206759654],\n",
       " 2: [0.06417697381356258,\n",
       "  0.06649793545780947,\n",
       "  0.0653287924955831,\n",
       "  0.06565819791547178,\n",
       "  0.06454937414042816,\n",
       "  0.06554151762108351,\n",
       "  0.0651695343093539,\n",
       "  0.0673171425065614,\n",
       "  0.06685060627098593,\n",
       "  0.06557358539465485,\n",
       "  0.0652233504041494,\n",
       "  0.06493069784876748,\n",
       "  0.06417661353169724,\n",
       "  0.06603098983406919,\n",
       "  0.06594141328602388],\n",
       " 3: [0.0628230919170325,\n",
       "  0.06460798546864066,\n",
       "  0.0626734235476627,\n",
       "  0.06462868811199562,\n",
       "  0.06378965445613438,\n",
       "  0.06266042008291761,\n",
       "  0.06481793818179671,\n",
       "  0.06393788631329189,\n",
       "  0.06380448833982999,\n",
       "  0.0642677156927699,\n",
       "  0.06297332483929785,\n",
       "  0.06416695162024011,\n",
       "  0.06391721020763175,\n",
       "  0.06574877446124494,\n",
       "  0.06436474660312619],\n",
       " 4: [0.06373153636491369,\n",
       "  0.06344177774546222,\n",
       "  0.06278401077400508,\n",
       "  0.06445115378408384,\n",
       "  0.061993022910087014,\n",
       "  0.06377432143798889,\n",
       "  0.06432555837715437,\n",
       "  0.0641052746310955,\n",
       "  0.0646952459117025,\n",
       "  0.06535606072303579,\n",
       "  0.06341851583858449,\n",
       "  0.06346631314262297,\n",
       "  0.06281277072048186,\n",
       "  0.0638121284554124,\n",
       "  0.06246114857352347],\n",
       " 5: [0.06384805546588568,\n",
       "  0.06306099204347348,\n",
       "  0.06204999851337489,\n",
       "  0.0627290421263278,\n",
       "  0.06317596391624869,\n",
       "  0.06440318258578037,\n",
       "  0.06433502802088253,\n",
       "  0.06408691126387012,\n",
       "  0.06345447433279405,\n",
       "  0.06330719136931817,\n",
       "  0.06417117270856776,\n",
       "  0.06467305313533356,\n",
       "  0.0642321907312362,\n",
       "  0.06296407461216147,\n",
       "  0.06422048185926765],\n",
       " 6: [0.06151338923600069,\n",
       "  0.06336046964217223,\n",
       "  0.06401293048898349,\n",
       "  0.06360349951447575,\n",
       "  0.06355404664656172,\n",
       "  0.06277506018466572,\n",
       "  0.0642297262577678,\n",
       "  0.062253740108782485,\n",
       "  0.06287571992211538,\n",
       "  0.06270086729554059,\n",
       "  0.06452074021403464,\n",
       "  0.06252746330535763,\n",
       "  0.06339732208297655,\n",
       "  0.06468076925607646,\n",
       "  0.06358727986653456],\n",
       " 7: [0.062108433504932115,\n",
       "  0.06215301473890633,\n",
       "  0.06284603305592729,\n",
       "  0.06244080228355864,\n",
       "  0.06213601869996799,\n",
       "  0.06350390332509759,\n",
       "  0.0622238345582139,\n",
       "  0.06258734162356971,\n",
       "  0.06215753517652001,\n",
       "  0.06345733881347115,\n",
       "  0.06203549772353245,\n",
       "  0.06323913341986831,\n",
       "  0.06181888291621929,\n",
       "  0.06120991712213175,\n",
       "  0.06158693816556504],\n",
       " 8: [0.06215533587651989,\n",
       "  0.06117313357653342,\n",
       "  0.061917668177824514,\n",
       "  0.06132126483083278,\n",
       "  0.06279754058722531,\n",
       "  0.06230810820453314,\n",
       "  0.06200054866427207,\n",
       "  0.0632003563880309,\n",
       "  0.062494003193483194,\n",
       "  0.063197650477964,\n",
       "  0.062396267521878546,\n",
       "  0.062457223727812594,\n",
       "  0.06238505958194729,\n",
       "  0.06177341400589276,\n",
       "  0.06176355771364783],\n",
       " 9: [0.06282505588836995,\n",
       "  0.061764382903706894,\n",
       "  0.061658944115637104,\n",
       "  0.06236657460228889,\n",
       "  0.06312769852185662,\n",
       "  0.06274519317413396,\n",
       "  0.06212118201969019,\n",
       "  0.06232244445232704,\n",
       "  0.0628906210991923,\n",
       "  0.06294286189461706,\n",
       "  0.06222200713676562,\n",
       "  0.06150055065597193,\n",
       "  0.06277942619652965,\n",
       "  0.06146886020239073,\n",
       "  0.06201090027894085]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropies_per_class # 15 entropy values for each class as each class has 15 subsamples of 500 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below, we just wish to confirm each subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-15.1934, -15.1389, -11.6547,  ...,  -9.2845, -10.4516, -11.7140],\n",
       "        [ 10.4669,   5.0653,   2.3566,  ...,   4.2572,   4.4443,  -4.6796],\n",
       "        [-10.3823,  -8.5444,  -5.3777,  ...,  -7.9962, -12.2085,  -9.7331],\n",
       "        ...,\n",
       "        [-15.6902,  -8.4537,  -9.8786,  ..., -12.3004, -12.9953,  -7.4532],\n",
       "        [-13.4329,  -9.8185, -14.7792,  ...,  -7.8993, -18.7558, -17.7395],\n",
       "        [-11.2296,  -6.3066,  -2.5546,  ...,  -6.4803,  -5.3827,  -5.1181]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_feature_space = []\n",
    "one_sample_from_class_3 = subsamples_per_class[3][14]\n",
    "\n",
    "for i in range(len(one_sample_from_class_3)):\n",
    "    img, label = one_sample_from_class_3[i]\n",
    "    output = modelCNN(to_device(img.unsqueeze(0), device))\n",
    "    one_sample_feature_space.append(features['18'])\n",
    "\n",
    "dataset_matrix = torch.cat(one_sample_feature_space, dim = 0).T \n",
    "dataset_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we want to select the most representative subsample of the 15 subsamples for each class, probably the most unrepresentative one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_worst_per_class(class_entropy, class_subset_entropy):\n",
    "    entropy_diff = {label:[np.abs(class_entropy[label] - class_subset_entropy[label][i])/class_entropy[label] for i in range(len(class_subset_entropy[label]))] for label in class_entropy}\n",
    "    print(entropy_diff)\n",
    "    min_max_entropy_diff_index = {label: [entropy_diff[label].index(min(entropy_diff[label])), entropy_diff[label].index(max(entropy_diff[label]))] for label in entropy_diff}\n",
    "    sum_best_entropy = sum([entropy_diff[label][min_max_entropy_diff_index[label][0]] for label in entropy_diff])\n",
    "    sum_worst_entropy =sum([entropy_diff[label][min_max_entropy_diff_index[label][1]] for label in entropy_diff])\n",
    "    return min_max_entropy_diff_index, sum_best_entropy, sum_worst_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0.01076592258817578, 0.01053577212583381, 0.00046944288571758135, 0.013751170084356541, 0.0208308412091394, 3.0852889652434957e-06, 0.010690320679122657, 0.011365193631067019, 0.009066605207247141, 0.017712581163416122, 0.008969278406600363, 0.03745590779739596, 0.011251912693606939, 0.00677623654785334, 0.0031037574591149713], 1: [0.009920071432388098, 0.01045105294693674, 0.0024404034358604795, 0.006546150584919258, 0.0017814214109252889, 0.01901369337419289, 0.009447467674968903, 0.02073280680717398, 0.0073964353507627745, 0.008091156179675225, 0.01732400370873462, 0.010518198436101196, 0.0008296953093291318, 0.020628192823733526, 0.0054935902230588435], 2: [0.011988618693397554, 0.023742834253475436, 0.005743753205966605, 0.010814984904419194, 0.006255475799898566, 0.00901867928295571, 0.0032919563821120403, 0.036354614456213144, 0.029172239171900715, 0.009512366089592795, 0.004120460921916028, 0.0003849564934605173, 0.01199416527127874, 0.016554156394545164, 0.015175115849192712], 3: [0.005165476066700063, 0.02309919019695291, 0.007535548224029113, 0.023427026725174036, 0.01014051658818357, 0.007741464476547321, 0.026423894554340216, 0.01248784086890812, 0.010375418424022722, 0.017710851129936984, 0.0027864639369192456, 0.01611520253853779, 0.012160424578121774, 0.04116414433618299, 0.01924738326430877], 4: [0.016179325563556005, 0.011559215406404392, 0.0010713276893073713, 0.02765339923862108, 0.011540725305663758, 0.016861520112465108, 0.02565081993193845, 0.022138464804941214, 0.031545371529994634, 0.0420818560964134, 0.01118831160915412, 0.011950424606139954, 0.0015298960007109359, 0.01746434112790419, 0.004076608667613577], 5: [0.0056112416955802265, 0.006785061053655259, 0.022708278318930512, 0.012013294958757987, 0.00497424619174745, 0.01435453181244942, 0.013281092752574587, 0.009373236854941937, 0.0005876880771157441, 0.002907404828093713, 0.010700360373601532, 0.018605011430865525, 0.011661398405123272, 0.008311517227339301, 0.011476982870566522], 6: [0.019173202276055307, 0.010278368876580833, 0.020681812597799238, 0.014153463622022168, 0.013364941014557225, 0.0009440549866320763, 0.024138606351123505, 0.007368358082594575, 0.0025490676378403976, 0.00023894557240565148, 0.028778804044069684, 0.003003860374143813, 0.010865978532809168, 0.0313304562095278, 0.013894842441366243], 7: [0.001398919599604245, 0.0006821269532609671, 0.010460463530194598, 0.003945021676928673, 0.0009553952018476847, 0.021037931427603306, 0.00045653882984367624, 0.006301132353019074, 0.0006094457788993509, 0.020289250950663217, 0.0025716065600597383, 0.0167808684411657, 0.0060544150678040545, 0.015845579738772708, 0.009783703757368832], 8: [9.856531522647016e-06, 0.015792680245781527, 0.003813950997193867, 0.013409413994047024, 0.010342212258667844, 0.002467792469991775, 0.002480496641378967, 0.0168230680933541, 0.0054586349875059715, 0.016779533025935815, 0.003886177312535897, 0.004866894509075754, 0.0037058540909715157, 0.006134839328714324, 0.006293416050322932], 9: [0.0033987081401907135, 0.0135416333767538, 0.01522562939141837, 0.003923840541303555, 0.008232308734327018, 0.0021231956371004354, 0.00784308258390879, 0.004628657032679762, 0.0044458707237780525, 0.005280224882325273, 0.00623277295837769, 0.017755381105261443, 0.0026699419942959454, 0.018261519297008606, 0.0096044269175447]}\n"
     ]
    }
   ],
   "source": [
    "min_max_entropy_diff_index, sum_best_entropy, sum_worst_entropy = find_best_worst_per_class(entropy, entropies_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [5, 11],\n",
       " 1: [12, 7],\n",
       " 2: [11, 7],\n",
       " 3: [10, 13],\n",
       " 4: [2, 9],\n",
       " 5: [8, 2],\n",
       " 6: [9, 13],\n",
       " 7: [6, 5],\n",
       " 8: [0, 7],\n",
       " 9: [5, 13]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_entropy_diff_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.008491753365969664, 0.2879505828398038)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_best_entropy, sum_worst_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We aggregate the most representative subsamples and most unrepresentative subsamples of each class to train CNN and observe the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_best_worst_subsets(grid_subsets, entropy_diff_index):\n",
    "    best_subsets = ConcatDataset([grid_subsets[label][entropy_diff_index[label][0]] for label in grid_subsets])\n",
    "    worst_subsets = ConcatDataset([grid_subsets[label][entropy_diff_index[label][1]] for label in grid_subsets])\n",
    "    return best_subsets, worst_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_agg_subsets, worst_agg_subsets = aggregate_best_worst_subsets(subsamples_per_class, min_max_entropy_diff_index)\n",
    "len(best_agg_subsets), len(worst_agg_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now since we have the most representative and unrepresentative subsets of 5000 images, we can use them to train a CNN submodel respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_size = 5000\n",
    "train_size = len(training_dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(training_dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "train_loader = DeviceDataLoader(DataLoader(training_dataset, batch_size*2), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.2609, val_loss: 2.2335, val_acc: 0.1522\n",
      "Epoch [1], train_loss: 2.0879, val_loss: 1.9846, val_acc: 0.2648\n",
      "Epoch [2], train_loss: 1.9189, val_loss: 1.8354, val_acc: 0.3145\n",
      "Epoch [3], train_loss: 1.8022, val_loss: 1.8422, val_acc: 0.3288\n",
      "Epoch [4], train_loss: 1.7097, val_loss: 1.6809, val_acc: 0.3803\n",
      "Epoch [5], train_loss: 1.5933, val_loss: 1.5617, val_acc: 0.4241\n",
      "Epoch [6], train_loss: 1.5393, val_loss: 1.5829, val_acc: 0.4002\n",
      "Epoch [7], train_loss: 1.4856, val_loss: 1.4778, val_acc: 0.4595\n",
      "Epoch [8], train_loss: 1.3896, val_loss: 1.4455, val_acc: 0.4715\n",
      "Epoch [9], train_loss: 1.2760, val_loss: 1.3918, val_acc: 0.5002\n",
      "Epoch [10], train_loss: 1.2075, val_loss: 1.4741, val_acc: 0.4867\n",
      "Epoch [11], train_loss: 1.1719, val_loss: 1.4374, val_acc: 0.4833\n",
      "Epoch [12], train_loss: 1.0709, val_loss: 1.4585, val_acc: 0.5053\n",
      "Epoch [13], train_loss: 0.9553, val_loss: 1.5401, val_acc: 0.4952\n",
      "Epoch [14], train_loss: 0.8695, val_loss: 1.3428, val_acc: 0.5545\n",
      "{'val_loss': 1.4250679016113281, 'val_acc': 0.526074230670929} {'val_loss': 1.3465557098388672, 'val_acc': 0.5521045923233032}\n"
     ]
    }
   ],
   "source": [
    "cifar10bestsub = DeviceDataLoader(DataLoader(best_agg_subsets, batch_size = batch_size, shuffle = True, pin_memory = True), device)\n",
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "best_model = to_device(Cifar10CnnModel(), device)\n",
    "history = fit(num_epochs, lr, best_model, cifar10bestsub, val_dl, opt_func)\n",
    "\n",
    "print(evaluate(best_model, test_loader), evaluate(best_model, train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.2934, val_loss: 2.2859, val_acc: 0.0998\n",
      "Epoch [1], train_loss: 2.1561, val_loss: 2.0143, val_acc: 0.2347\n",
      "Epoch [2], train_loss: 1.9831, val_loss: 1.9446, val_acc: 0.2655\n",
      "Epoch [3], train_loss: 1.8467, val_loss: 1.7867, val_acc: 0.3266\n",
      "Epoch [4], train_loss: 1.7517, val_loss: 1.7146, val_acc: 0.3662\n",
      "Epoch [5], train_loss: 1.6659, val_loss: 1.7688, val_acc: 0.3636\n",
      "Epoch [6], train_loss: 1.6301, val_loss: 1.6754, val_acc: 0.3820\n",
      "Epoch [7], train_loss: 1.5895, val_loss: 1.5810, val_acc: 0.4265\n",
      "Epoch [8], train_loss: 1.5206, val_loss: 1.6176, val_acc: 0.4207\n",
      "Epoch [9], train_loss: 1.4638, val_loss: 1.5356, val_acc: 0.4390\n",
      "Epoch [10], train_loss: 1.3641, val_loss: 1.5050, val_acc: 0.4707\n",
      "Epoch [11], train_loss: 1.2824, val_loss: 1.4824, val_acc: 0.4660\n",
      "Epoch [12], train_loss: 1.2376, val_loss: 1.4274, val_acc: 0.4853\n",
      "Epoch [13], train_loss: 1.1635, val_loss: 1.3769, val_acc: 0.5094\n",
      "Epoch [14], train_loss: 1.1185, val_loss: 1.4235, val_acc: 0.5000\n",
      "{'val_loss': 1.456299901008606, 'val_acc': 0.49882811307907104} {'val_loss': 1.4079854488372803, 'val_acc': 0.5085100531578064}\n"
     ]
    }
   ],
   "source": [
    "cifar10worstsub = DeviceDataLoader(DataLoader(worst_agg_subsets, batch_size = batch_size, shuffle = True, pin_memory = True), device)\n",
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "worst_model = to_device(Cifar10CnnModel(), device)\n",
    "history = fit(num_epochs, lr, worst_model, cifar10worstsub, val_dl, opt_func)\n",
    "\n",
    "print(evaluate(worst_model, test_loader), evaluate(worst_model, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not finished yet, will update later this week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06589155]]\n"
     ]
    }
   ],
   "source": [
    "feature_space_good_subsamples = []\n",
    "for i in range(len(best_agg_subsets)):\n",
    "    img, label = best_agg_subsets[i]\n",
    "    output = modelCNN(to_device(img.unsqueeze(0), device))\n",
    "    feature_space_good_subsamples.append(features['18'])\n",
    "\n",
    "dataset_matrix1 = torch.cat(feature_space_good_subsamples, dim = 0).T \n",
    "print(BSIE(torch.svd(dataset_matrix1)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06571683]]\n"
     ]
    }
   ],
   "source": [
    "feature_space_bad_subsamples = []\n",
    "for i in range(len(worst_agg_subsets)):\n",
    "    img, label = worst_agg_subsets[i]\n",
    "    output = modelCNN(to_device(img.unsqueeze(0), device))\n",
    "    feature_space_bad_subsamples.append(features['18'])\n",
    "\n",
    "dataset_matrix2 = torch.cat(feature_space_bad_subsamples, dim = 0).T \n",
    "print(BSIE(torch.svd(dataset_matrix2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: (0, 1000), 1:(1000, 2000), 2:(2000, 3000), 3:(3000, 4000), 4:(4000, 5000), 5:(5000, 6000), 6:(6000, 7000), 7:(7000, 8000), 8:(8000, 9000), 9:(9000, 10000)}\n",
    "testing_data_by_class = {category: Subset(ImageFolder('./data/cifar10/test/', transform=ToTensor()), range(classes[category][0], classes[category][1])) for category in classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_good_model_by_training_class = {label: evaluate(best_model, DeviceDataLoader(DataLoader(training_data_by_class[label], batch_size*2), device))['val_acc'] for label in training_data_by_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_bad_model_by_training_class = {label: evaluate(worst_model, DeviceDataLoader(DataLoader(training_data_by_class[label], batch_size*2), device))['val_acc'] for label in training_data_by_class}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5671185255050659,\n",
       " 1: 0.7796415090560913,\n",
       " 2: 0.37470129132270813,\n",
       " 3: 0.4009995460510254,\n",
       " 4: 0.5318244695663452,\n",
       " 5: 0.4244140684604645,\n",
       " 6: 0.6315027475357056,\n",
       " 7: 0.48544350266456604,\n",
       " 8: 0.7400505542755127,\n",
       " 9: 0.5857077240943909}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_good_model_by_training_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6469324827194214,\n",
       " 1: 0.7410386204719543,\n",
       " 2: 0.4079618453979492,\n",
       " 3: 0.5611327886581421,\n",
       " 4: 0.34088924527168274,\n",
       " 5: 0.1778377741575241,\n",
       " 6: 0.48367413878440857,\n",
       " 7: 0.500160813331604,\n",
       " 8: 0.6454044580459595,\n",
       " 9: 0.5766544342041016}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_bad_model_by_training_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Classes': evaluation_good_model_by_training_class.keys(), 'Good Model Training Accuracy': evaluation_good_model_by_training_class.values()})\n",
    "df2 = pd.DataFrame({'Classes': evaluation_bad_model_by_training_class.keys(), 'Bad Model Training Accuracy': evaluation_bad_model_by_training_class.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Good Model Training Accuracy</th>\n",
       "      <th>Bad Model Training Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.567119</td>\n",
       "      <td>0.646932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.779642</td>\n",
       "      <td>0.741039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.374701</td>\n",
       "      <td>0.407962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.561133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.531824</td>\n",
       "      <td>0.340889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.424414</td>\n",
       "      <td>0.177838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.631503</td>\n",
       "      <td>0.483674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.485444</td>\n",
       "      <td>0.500161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.740051</td>\n",
       "      <td>0.645404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.585708</td>\n",
       "      <td>0.576654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classes  Good Model Training Accuracy  Bad Model Training Accuracy\n",
       "0        0                      0.567119                     0.646932\n",
       "1        1                      0.779642                     0.741039\n",
       "2        2                      0.374701                     0.407962\n",
       "3        3                      0.401000                     0.561133\n",
       "4        4                      0.531824                     0.340889\n",
       "5        5                      0.424414                     0.177838\n",
       "6        6                      0.631503                     0.483674\n",
       "7        7                      0.485444                     0.500161\n",
       "8        8                      0.740051                     0.645404\n",
       "9        9                      0.585708                     0.576654"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df1, df2, on = 'Classes')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data = df[['Good Model Training Accuracy', 'Bad Model Training Accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.549467921257019,\n",
       " 1: 0.7599676847457886,\n",
       " 2: 0.33816003799438477,\n",
       " 3: 0.3820379972457886,\n",
       " 4: 0.5064655542373657,\n",
       " 5: 0.42046067118644714,\n",
       " 6: 0.6182987689971924,\n",
       " 7: 0.47979527711868286,\n",
       " 8: 0.7061893939971924,\n",
       " 9: 0.5652613043785095}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_good_model_by_testing_class = {label: evaluate(best_model, DeviceDataLoader(DataLoader(testing_data_by_class[label], batch_size*2), device))['val_acc'] for label in testing_data_by_class}\n",
    "evaluation_good_model_by_testing_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6415342092514038,\n",
       " 1: 0.723296046257019,\n",
       " 2: 0.3957098722457886,\n",
       " 3: 0.5420258641242981,\n",
       " 4: 0.3447939157485962,\n",
       " 5: 0.19302263855934143,\n",
       " 6: 0.4910088777542114,\n",
       " 7: 0.4904364347457886,\n",
       " 8: 0.6324083805084229,\n",
       " 9: 0.5320245027542114}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_bad_model_by_testing_class = {label: evaluate(worst_model, DeviceDataLoader(DataLoader(testing_data_by_class[label], batch_size*2), device))['val_acc'] for label in testing_data_by_class}\n",
    "evaluation_bad_model_by_testing_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Classes': evaluation_good_model_by_testing_class.keys(), 'Good Model Testing Accuracy': evaluation_good_model_by_testing_class.values()})\n",
    "df2 = pd.DataFrame({'Classes': evaluation_bad_model_by_testing_class.keys(), 'Bad Model Testing Accuracy': evaluation_bad_model_by_testing_class.values()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Good Model Testing Accuracy</th>\n",
       "      <th>Bad Model Testing Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.549468</td>\n",
       "      <td>0.641534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.759968</td>\n",
       "      <td>0.723296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.338160</td>\n",
       "      <td>0.395710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.382038</td>\n",
       "      <td>0.542026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.506466</td>\n",
       "      <td>0.344794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.420461</td>\n",
       "      <td>0.193023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.618299</td>\n",
       "      <td>0.491009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.479795</td>\n",
       "      <td>0.490436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.706189</td>\n",
       "      <td>0.632408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.565261</td>\n",
       "      <td>0.532025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classes  Good Model Testing Accuracy  Bad Model Testing Accuracy\n",
       "0        0                     0.549468                    0.641534\n",
       "1        1                     0.759968                    0.723296\n",
       "2        2                     0.338160                    0.395710\n",
       "3        3                     0.382038                    0.542026\n",
       "4        4                     0.506466                    0.344794\n",
       "5        5                     0.420461                    0.193023\n",
       "6        6                     0.618299                    0.491009\n",
       "7        7                     0.479795                    0.490436\n",
       "8        8                     0.706189                    0.632408\n",
       "9        9                     0.565261                    0.532025"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df1, df2, on = 'Classes')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(df[['Good Model Testing Accuracy', 'Bad Model Testing Accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
