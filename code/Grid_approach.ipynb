{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Approach to enhance or eliminate bias in the training subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split, DataLoader, SubsetRandomSampler, Subset, ConcatDataset\n",
    "import os\n",
    "from Truncate import truncate\n",
    "from BSI_Entropy import BSIE\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Like what we did in the original file, we set up all the classes/functions properly and prepare our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/cifar10'\n",
    "training_dataset = ImageFolder(data_dir+'/train', transform=ToTensor())\n",
    "test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CnnModel(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now create a CNN model object from the class above and assign it trained parameters we stored. This CNN model will be our embedding function to help us extract feature space of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN = to_device(Cifar10CnnModel(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.load_state_dict(torch.load('cifar10-cnn.pth')) # load our pretrained model parameters and assign to the new model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.9372484087944031, 'val_acc': 0.7689453363418579}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size=128\n",
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "evaluate(modelCNN, test_loader) # this will make sure it is the same CNN model we trained initially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This following function will help us to attain intermediate features of images before the final classification layer of the CNN model which is our embedding function in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x17914098650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.network[18].register_forward_hook(get_features('18'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following code, we attain the intermediate feature space for each class in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_feature_data = {label: [] for label in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in training_dataset:\n",
    "    if label in class_feature_data:\n",
    "        output = modelCNN(to_device(image.unsqueeze(0), device))\n",
    "        class_feature_data[label].append(features['18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in class_feature_data:\n",
    "    class_feature_data[label] = torch.cat(class_feature_data[label], dim = 0).T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We then apply SVD to the feature space of each training class to attain singular vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([8305.8789, 1585.3182, 1073.5406,  963.0099,  828.8642,  769.5621,\n",
      "         616.4266,  580.3412,  516.1616,  488.6954,  402.4275,  341.2084,\n",
      "         275.5750,  266.5753,  257.8827,  231.7271,  198.1219,  183.4829,\n",
      "         180.9243,  177.5941,  156.9813,  144.6703,  141.9810,  132.6933,\n",
      "         126.5861,  122.6644,  117.9465,  113.8699,  110.0822,  106.3363,\n",
      "         103.7124,   93.6782,   91.7617,   89.5256,   84.1956,   82.4179,\n",
      "          80.3859,   78.2227,   75.4202,   72.8308,   71.0707,   69.9203,\n",
      "          65.3629,   64.8393,   62.6921,   60.4378,   58.1620,   56.9152,\n",
      "          54.7706,   54.1711,   52.7214,   51.6815,   50.4430,   49.4335,\n",
      "          47.0124,   45.7740,   43.8802,   43.6093,   42.2480,   40.5315,\n",
      "          39.1501,   37.6497,   36.0807,   34.7374]), 1: tensor([12095.2061,  1403.4337,  1103.4655,   963.6672,   940.5552,   853.0527,\n",
      "          744.2352,   670.6514,   567.7023,   496.4117,   435.3126,   407.3419,\n",
      "          350.2057,   277.2058,   250.3987,   228.0301,   213.1301,   201.9636,\n",
      "          183.1754,   177.0799,   173.2148,   157.2500,   154.9059,   141.8093,\n",
      "          136.7393,   130.5036,   123.9009,   111.7951,   109.5792,   104.0679,\n",
      "          102.0208,    98.5726,    95.6230,    92.5422,    91.4762,    87.7978,\n",
      "           85.6393,    80.0061,    75.8346,    74.1482,    72.9685,    70.6738,\n",
      "           68.0645,    67.4477,    63.2545,    62.6573,    58.7270,    57.1137,\n",
      "           55.9109,    54.9257,    53.4217,    51.6753,    50.8225,    49.1771,\n",
      "           46.7251,    46.5821,    44.0645,    43.7546,    42.5789,    39.9152,\n",
      "           38.9562,    36.2970,    35.4175,    31.8563]), 2: tensor([6268.6768,  996.6603,  861.0262,  768.5312,  739.1451,  694.6696,\n",
      "         600.1634,  527.4907,  457.0351,  414.4943,  364.2594,  286.4704,\n",
      "         252.5366,  232.2138,  215.6590,  201.4022,  193.2244,  170.9879,\n",
      "         168.6637,  152.4425,  139.6423,  135.0280,  128.0127,  120.3685,\n",
      "         113.6636,  107.9870,  103.6013,  101.6421,   95.3172,   93.9559,\n",
      "          88.8051,   85.6524,   82.2905,   80.0169,   76.2634,   74.8454,\n",
      "          73.6839,   72.4649,   67.7688,   66.6625,   64.4223,   61.4686,\n",
      "          60.3447,   58.9813,   57.0994,   55.9028,   54.6610,   53.0541,\n",
      "          49.8968,   48.1705,   46.9777,   46.7670,   45.8985,   43.3955,\n",
      "          42.3475,   41.3480,   40.6217,   39.2341,   37.1279,   36.3377,\n",
      "          35.2792,   34.6068,   33.2892,   31.7638]), 3: tensor([6473.5073,  841.2786,  778.4031,  699.7131,  636.0625,  580.7430,\n",
      "         545.3973,  478.3984,  441.1371,  360.8887,  271.3305,  227.0016,\n",
      "         215.7848,  210.0773,  184.9863,  174.0165,  161.9877,  152.2511,\n",
      "         149.6021,  136.9679,  129.8981,  125.9189,  120.4214,  109.9679,\n",
      "         104.4016,  103.5947,   97.8948,   96.5096,   85.8014,   85.5063,\n",
      "          82.8391,   81.4511,   78.8962,   73.4542,   70.5081,   69.6882,\n",
      "          66.7658,   64.2578,   64.0274,   62.4119,   60.9270,   60.1115,\n",
      "          58.6312,   55.4811,   53.9831,   52.1251,   51.3327,   50.8141,\n",
      "          47.4976,   47.1265,   45.3364,   43.8578,   43.6088,   43.3220,\n",
      "          41.2991,   39.3377,   38.7576,   37.6581,   36.6142,   35.8798,\n",
      "          34.7457,   33.8410,   33.0287,   31.0110]), 4: tensor([7251.3999, 1030.9064,  909.2327,  707.2688,  637.4417,  578.5201,\n",
      "         508.1140,  466.0776,  446.0780,  397.8710,  320.4633,  269.8743,\n",
      "         264.0945,  229.7420,  226.3317,  194.3986,  188.6454,  166.8791,\n",
      "         154.9358,  152.1550,  140.9037,  124.5961,  121.1879,  118.2018,\n",
      "         111.4068,   99.5754,   99.0052,   96.0552,   93.1947,   88.4500,\n",
      "          83.6878,   81.2279,   77.7753,   77.0465,   75.4441,   72.9830,\n",
      "          69.8479,   68.6878,   66.5030,   65.3280,   62.9284,   60.9607,\n",
      "          57.7178,   55.7312,   55.0482,   51.6707,   50.9145,   48.9035,\n",
      "          48.0556,   47.2219,   46.6511,   44.7408,   42.6484,   41.6211,\n",
      "          40.9727,   39.4311,   38.1823,   37.1305,   35.6565,   34.5898,\n",
      "          33.4765,   32.3365,   31.2384,   30.4821]), 5: tensor([6615.5000,  935.8938,  749.0901,  681.4392,  659.0427,  591.3130,\n",
      "         568.2397,  471.9457,  434.5263,  397.9953,  320.4688,  229.8735,\n",
      "         207.7418,  199.3160,  181.2436,  179.3377,  164.5392,  158.5162,\n",
      "         144.6888,  135.9444,  125.6181,  122.4719,  114.7564,  108.4020,\n",
      "          98.2476,   96.3082,   94.3113,   87.3923,   86.2062,   83.2604,\n",
      "          80.6292,   78.4679,   76.5878,   72.7462,   71.2571,   69.3191,\n",
      "          67.5854,   64.7256,   61.7663,   59.2221,   57.9550,   57.0776,\n",
      "          55.5528,   53.5544,   52.0715,   51.1490,   49.4429,   48.2574,\n",
      "          47.1759,   45.2133,   44.9031,   42.5675,   41.7146,   40.3162,\n",
      "          39.2917,   38.8730,   38.4060,   36.5054,   34.4352,   33.8537,\n",
      "          32.6440,   32.3013,   31.1622,   29.4608]), 6: tensor([7264.2148, 1025.9622,  821.9323,  766.3347,  668.6025,  559.7619,\n",
      "         490.6772,  444.9507,  408.4172,  347.1433,  305.5564,  228.6729,\n",
      "         222.3128,  206.2740,  182.9453,  170.8316,  157.5761,  153.2848,\n",
      "         131.4534,  127.3159,  126.6383,  111.9518,  103.7711,  100.5675,\n",
      "          94.8066,   89.5136,   87.9663,   82.4812,   80.3807,   75.5823,\n",
      "          75.0115,   72.9168,   70.3978,   67.0040,   66.0912,   62.3881,\n",
      "          61.8958,   60.0585,   58.2270,   55.7178,   53.8061,   52.5783,\n",
      "          49.9245,   48.5092,   48.1664,   46.0542,   45.5001,   43.5996,\n",
      "          41.0822,   40.6236,   39.1936,   38.3900,   36.9234,   36.2563,\n",
      "          35.1064,   34.7974,   33.9532,   31.8801,   31.6158,   30.7459,\n",
      "          29.4570,   27.8902,   26.1976,   25.0586]), 7: tensor([9169.8828, 1493.6215, 1051.6710,  827.7892,  704.5131,  648.9139,\n",
      "         604.9308,  552.1254,  529.1401,  405.0483,  379.9564,  326.4211,\n",
      "         275.0515,  241.4179,  225.5518,  223.2711,  190.9410,  185.7654,\n",
      "         180.1633,  159.7839,  149.9531,  140.9736,  129.3146,  123.8428,\n",
      "         118.9251,  117.3516,  110.0507,  101.6755,  100.0895,   94.0084,\n",
      "          89.3221,   87.6476,   83.3122,   83.0177,   79.0291,   77.2495,\n",
      "          76.2627,   75.2523,   72.6804,   71.6437,   68.6938,   65.9932,\n",
      "          62.9971,   61.2922,   59.8471,   58.7325,   57.2511,   55.7114,\n",
      "          54.3630,   52.0995,   49.9387,   48.9176,   47.1538,   46.9003,\n",
      "          44.8975,   43.6781,   41.0143,   40.4583,   40.1294,   39.0184,\n",
      "          37.9315,   34.9119,   34.0490,   32.1201]), 8: tensor([9594.4805, 1124.0610, 1052.3362, 1004.5804,  869.7374,  671.5615,\n",
      "         621.2519,  523.7562,  475.3379,  443.6017,  403.6830,  317.1657,\n",
      "         286.7527,  249.1192,  224.5342,  216.5424,  185.4713,  184.0177,\n",
      "         174.7412,  168.6416,  144.3980,  139.9327,  130.4194,  128.2969,\n",
      "         119.3904,  116.7353,  110.4874,  104.6957,  102.1369,  100.8237,\n",
      "          93.3471,   87.9545,   87.0240,   81.4927,   76.3620,   74.8747,\n",
      "          72.8455,   72.1757,   69.9687,   67.7925,   63.9059,   63.6517,\n",
      "          61.2246,   57.5016,   57.1501,   54.6067,   54.3347,   52.6592,\n",
      "          51.7210,   50.4249,   48.6767,   46.9900,   45.1177,   43.7656,\n",
      "          41.3862,   39.9052,   39.1436,   38.6954,   36.9722,   35.7993,\n",
      "          34.9810,   33.8001,   32.5454,   30.0804]), 9: tensor([10423.7295,  1075.6924,  1005.2132,   868.2537,   845.2270,   768.0860,\n",
      "          653.5539,   533.2513,   448.2159,   377.9797,   349.4870,   321.1977,\n",
      "          258.0855,   248.5007,   222.5318,   209.2140,   185.0927,   176.9587,\n",
      "          170.2491,   150.6953,   141.7572,   133.9512,   126.0533,   120.6464,\n",
      "          114.3579,   106.9467,   101.3006,   100.5582,    96.6379,    94.3995,\n",
      "           91.8883,    90.4309,    87.2283,    83.8863,    80.6998,    77.5528,\n",
      "           76.5992,    75.3139,    74.1166,    70.6445,    68.6330,    63.5477,\n",
      "           61.1679,    60.1646,    56.9889,    55.9821,    54.9749,    54.2968,\n",
      "           52.9092,    51.9405,    50.1667,    48.8075,    46.8235,    46.0916,\n",
      "           45.4860,    44.6703,    41.4841,    40.2458,    37.5727,    36.9767,\n",
      "           36.2387,    35.1060,    33.5660,    30.2874])}\n"
     ]
    }
   ],
   "source": [
    "singular_vector = {label: torch.svd(class_feature_data[label])[1] for label in class_feature_data}\n",
    "print(singular_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note here, we don't truncate singular vector anymore to get BSIE value because the singular vector of each training class feature space contains values apparently greater than 0 almost at each entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.06290901692797712, 1: 0.06283993894178475, 2: 0.06495570296841247, 3: 0.06314928805309994, 4: 0.06271682050760996, 5: 0.06349178769941977, 6: 0.06271585297092763, 7: 0.062195439924848994, 8: 0.06215472324653093, 9: 0.0626122551072612}\n"
     ]
    }
   ],
   "source": [
    "entropy = {label: BSIE(singular_vector[label]).item() for label in singular_vector}\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to select representative and unrepresentative subsamples for each class and combine them together to train CNN submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {0: (0, 5000), 1:(5000, 10000), 2:(10000, 15000), 3:(15000, 20000), 4:(20000, 25000), 5:(25000, 30000), 6:(30000, 35000), 7:(35000, 40000), 8:(40000, 45000), 9:(45000, 50000)}\n",
    "training_data_by_class = {category: Subset(ImageFolder('./data/cifar10/train/', transform=ToTensor()), range(classes[category][0], classes[category][1])) for category in classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_subsample_per_class(classified_training_data, num = 15, n_sample = 500):\n",
    "    subsample = {j: [random_split(classified_training_data[j], [n_sample, len(classified_training_data[j]) - n_sample])[0] for round in range(num)] for j in classified_training_data}\n",
    "    return subsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9137, 0.9059, 0.9098,  ..., 0.9333, 0.9255, 0.9216],\n",
       "         [0.9059, 0.8980, 0.9020,  ..., 0.9294, 0.9255, 0.9176],\n",
       "         [0.9098, 0.9020, 0.9059,  ..., 0.9333, 0.9294, 0.9255],\n",
       "         ...,\n",
       "         [0.8667, 0.8549, 0.8588,  ..., 0.8784, 0.8706, 0.8667],\n",
       "         [0.8588, 0.8471, 0.8510,  ..., 0.8706, 0.8667, 0.8627],\n",
       "         [0.8510, 0.8431, 0.8471,  ..., 0.8667, 0.8667, 0.8588]],\n",
       "\n",
       "        [[0.9098, 0.9020, 0.9059,  ..., 0.9294, 0.9294, 0.9216],\n",
       "         [0.9020, 0.8941, 0.8980,  ..., 0.9255, 0.9255, 0.9176],\n",
       "         [0.9059, 0.8980, 0.9020,  ..., 0.9333, 0.9294, 0.9255],\n",
       "         ...,\n",
       "         [0.8431, 0.8314, 0.8353,  ..., 0.8706, 0.8627, 0.8588],\n",
       "         [0.8353, 0.8235, 0.8275,  ..., 0.8627, 0.8588, 0.8549],\n",
       "         [0.8275, 0.8196, 0.8235,  ..., 0.8588, 0.8588, 0.8510]],\n",
       "\n",
       "        [[0.8902, 0.8902, 0.8980,  ..., 0.9294, 0.9294, 0.9216],\n",
       "         [0.8863, 0.8784, 0.8863,  ..., 0.9255, 0.9255, 0.9176],\n",
       "         [0.8902, 0.8824, 0.8863,  ..., 0.9333, 0.9294, 0.9255],\n",
       "         ...,\n",
       "         [0.8392, 0.8314, 0.8353,  ..., 0.8745, 0.8706, 0.8627],\n",
       "         [0.8353, 0.8235, 0.8275,  ..., 0.8667, 0.8627, 0.8588],\n",
       "         [0.8275, 0.8196, 0.8235,  ..., 0.8627, 0.8627, 0.8549]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsamples_per_class = num_subsample_per_class(training_data_by_class)\n",
    "subsamples_per_class[2][13][499][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_space(dictionary):\n",
    "    feature_space_dict = {label: [[] for _ in range(len(dictionary[label]))] for label in dictionary}\n",
    "    for category in dictionary:\n",
    "        for i in range(len(dictionary[category])):\n",
    "            subsample = dictionary[category][i]\n",
    "            for img, lbl in subsample:\n",
    "                if lbl in feature_space_dict:\n",
    "                    output = modelCNN(to_device(img.unsqueeze(0), device))\n",
    "                    feature_space_dict[lbl][i].append(features['18'])\n",
    "\n",
    "            feature_space_dict[category][i] = torch.cat(feature_space_dict[category][i], dim = 0).T\n",
    "\n",
    "    return feature_space_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, we want to confirm we extract a list of feature space for each class 0 to 9 correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space_per_class = feature_space(subsamples_per_class)#[3][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 500])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_space_per_class[3][14].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10.4720, -12.0392,  -6.2620,  ..., -19.7578, -16.0811, -22.5578],\n",
       "        [  7.0483,  -8.4155,   3.7559,  ..., -16.7447,  -2.4426,   8.0573],\n",
       "        [ -6.7543,  -6.8985,  -7.8737,  ..., -10.5969,  -6.1091, -10.5488],\n",
       "        ...,\n",
       "        [ -9.8876, -11.9938, -16.3818,  ..., -15.9279,  -7.8179, -11.7522],\n",
       "        [-10.7643, -12.9664, -18.5365,  ..., -19.1223,  -9.7846, -12.0340],\n",
       "        [ -3.5994,  -4.4575,  -4.9504,  ...,  -8.8697,  -5.4898,  -4.8947]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_space_per_class[3][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to singular vectors\n",
    "def entropy_values_per_class(dict):\n",
    "    return {label: [BSIE(torch.svd(dict[label][i])[1]).item() for i in range(len(dict[label]))] for label in dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_per_class = entropy_values_per_class(feature_space_per_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.06439395609530962,\n",
       "  0.06292441962248518,\n",
       "  0.06359862810676586,\n",
       "  0.06362807091416656,\n",
       "  0.06274896601813384,\n",
       "  0.06429198410638515,\n",
       "  0.06369383492051284,\n",
       "  0.0632180096030941,\n",
       "  0.06397535133417398,\n",
       "  0.06295952256733506,\n",
       "  0.06411708451153308,\n",
       "  0.06307028822368876,\n",
       "  0.06402714204867299,\n",
       "  0.06376201956790517,\n",
       "  0.06341724388991532],\n",
       " 1: [0.061787274771073486,\n",
       "  0.062338574506456745,\n",
       "  0.061420278651014626,\n",
       "  0.062023171278363654,\n",
       "  0.06187684033652663,\n",
       "  0.061814124572514784,\n",
       "  0.062222152783827256,\n",
       "  0.0625186695005735,\n",
       "  0.0622038811690977,\n",
       "  0.06167997572019335,\n",
       "  0.06281738835150508,\n",
       "  0.06264392127265628,\n",
       "  0.061521433347196064,\n",
       "  0.06220526068110588,\n",
       "  0.0624334397412466],\n",
       " 2: [0.06443611976374886,\n",
       "  0.06560658777050044,\n",
       "  0.06522065161595636,\n",
       "  0.06532914793667866,\n",
       "  0.06559189326477843,\n",
       "  0.06577518870257715,\n",
       "  0.0660935019043406,\n",
       "  0.0667085081863984,\n",
       "  0.0653809321747777,\n",
       "  0.06509328707673245,\n",
       "  0.0668100498183215,\n",
       "  0.06541731207121182,\n",
       "  0.06557156464085556,\n",
       "  0.06589384091216766,\n",
       "  0.06517080908242467],\n",
       " 3: [0.06458273146233517,\n",
       "  0.06293969442146075,\n",
       "  0.06349797419477443,\n",
       "  0.06371725848183774,\n",
       "  0.06323748095560477,\n",
       "  0.06408443499492644,\n",
       "  0.06593252467458122,\n",
       "  0.06316219936936052,\n",
       "  0.0657622678394787,\n",
       "  0.06463515170600365,\n",
       "  0.06438056453827234,\n",
       "  0.06297098433291681,\n",
       "  0.0646404166468747,\n",
       "  0.06467916708171484,\n",
       "  0.06509827399139911],\n",
       " 4: [0.06354353478647268,\n",
       "  0.0625009838298628,\n",
       "  0.06315088917787537,\n",
       "  0.06328236505356999,\n",
       "  0.06268683795599705,\n",
       "  0.06451138558654068,\n",
       "  0.0631399550155819,\n",
       "  0.06284667228954344,\n",
       "  0.06240879603553651,\n",
       "  0.06334201941613427,\n",
       "  0.06329462937424923,\n",
       "  0.06180596083765877,\n",
       "  0.06440767558582539,\n",
       "  0.06408283533629977,\n",
       "  0.0639661495606626],\n",
       " 5: [0.06397126305997569,\n",
       "  0.06321729312053137,\n",
       "  0.06436516248887347,\n",
       "  0.0624769648754564,\n",
       "  0.06361995333161441,\n",
       "  0.0634296920368822,\n",
       "  0.06226769161865775,\n",
       "  0.06426311357727965,\n",
       "  0.06460122745978591,\n",
       "  0.06367422685925239,\n",
       "  0.06340052118465878,\n",
       "  0.06691560530268814,\n",
       "  0.0627636022914202,\n",
       "  0.06537855105540757,\n",
       "  0.06250486660512411],\n",
       " 6: [0.06338767006965573,\n",
       "  0.0627715822933631,\n",
       "  0.06337992711792173,\n",
       "  0.06350200811297779,\n",
       "  0.06246255156996283,\n",
       "  0.06432976391641243,\n",
       "  0.06236205329826083,\n",
       "  0.06294884030734682,\n",
       "  0.061854136792914405,\n",
       "  0.06352733741175454,\n",
       "  0.06385298548700247,\n",
       "  0.06363339022087144,\n",
       "  0.06372480833095961,\n",
       "  0.0634196901072831,\n",
       "  0.06356937453469769],\n",
       " 7: [0.06389296727485838,\n",
       "  0.061992580710077405,\n",
       "  0.06120573734410151,\n",
       "  0.06310204298245892,\n",
       "  0.06252824203273921,\n",
       "  0.06264288032932264,\n",
       "  0.06318132215429595,\n",
       "  0.06292132845420884,\n",
       "  0.062443702010127056,\n",
       "  0.06202052776487876,\n",
       "  0.061769274233272164,\n",
       "  0.061495204568022266,\n",
       "  0.06274711941074251,\n",
       "  0.06200787214922643,\n",
       "  0.06230571416055897],\n",
       " 8: [0.06241095833091126,\n",
       "  0.06300008560463333,\n",
       "  0.06140515736329266,\n",
       "  0.06245534111340156,\n",
       "  0.06266066575630602,\n",
       "  0.061746233413117024,\n",
       "  0.06153271854424347,\n",
       "  0.06311249050945544,\n",
       "  0.06276485585180192,\n",
       "  0.06279675059575263,\n",
       "  0.06294484012683188,\n",
       "  0.06259006926904209,\n",
       "  0.06223639815648674,\n",
       "  0.061720787870295335,\n",
       "  0.0624152202127356],\n",
       " 9: [0.0621477086211788,\n",
       "  0.06128380311073578,\n",
       "  0.06173870917740065,\n",
       "  0.06249168876258382,\n",
       "  0.06266614004143889,\n",
       "  0.06181573616452685,\n",
       "  0.06169997875325228,\n",
       "  0.06199111050799355,\n",
       "  0.06254228371937232,\n",
       "  0.0627245349622102,\n",
       "  0.06253488459224743,\n",
       "  0.06256282338217123,\n",
       "  0.061534234818355626,\n",
       "  0.06303446284050873,\n",
       "  0.06341549768365606]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropies_per_class # 15 entropy values for each class as each class has 15 subsamples of 500 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we want to select the most representative subsample of the 15 subsamples for each class, probably the most unrepresentative one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_worst_per_class(class_entropy, class_subset_entropy):\n",
    "    entropy_diff = {label:[np.abs(class_entropy[label] - class_subset_entropy[label][i])/class_entropy[label] for i in range(len(class_subset_entropy[label]))] for label in class_entropy}\n",
    "    print(entropy_diff)\n",
    "    min_max_entropy_diff_index = {label: [entropy_diff[label].index(min(entropy_diff[label])), entropy_diff[label].index(max(entropy_diff[label]))] for label in entropy_diff}\n",
    "    sum_best_entropy = sum([entropy_diff[label][min_max_entropy_diff_index[label][0]] for label in entropy_diff])\n",
    "    sum_worst_entropy =sum([entropy_diff[label][min_max_entropy_diff_index[label][1]] for label in entropy_diff])\n",
    "    return min_max_entropy_diff_index, sum_best_entropy, sum_worst_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [0.0236045520951721, 0.00024484080756966855, 0.010962040299854918, 0.011430062355173328, 0.0025441648536094126, 0.021983608168465674, 0.012475445188314276, 0.004911739051187651, 0.016950422344346506, 0.000802836251848547, 0.019203409027024623, 0.0025635640737523445, 0.01777368611523495, 0.013559306464836779, 0.008078761785771537], 1: [0.016751514855647028, 0.00797843606742627, 0.022591687940456153, 0.012997588431423363, 0.015326218030707137, 0.01632424197961557, 0.009831106910046648, 0.0051125040320117564, 0.010121871271649324, 0.018459012550378098, 0.0003588576096574431, 0.003119316670725276, 0.020981968104872795, 0.010099918481251809, 0.006468803238569809], 2: [0.007999039051525306, 0.010020441198280786, 0.004078912788808356, 0.005749225259678798, 0.00979421770980361, 0.012616070594497188, 0.017516536407610587, 0.026984623949621617, 0.006546449148152793, 0.0021181220744681075, 0.028547868242004795, 0.007106521547828242, 0.009481256368552982, 0.014442734061571176, 0.0033115816499871263], 3: [0.02269928060043852, 0.0033190181251600754, 0.00552161635426982, 0.008994090768849375, 0.0013965779381499498, 0.014808511238324214, 0.04407391923628585, 0.00020445703599573386, 0.04137781860947657, 0.0235293809116945, 0.019497868038307414, 0.002823527005295721, 0.0236127538369225, 0.024226386009743774, 0.030863149821425434], 4: [0.013181699457522797, 0.0034414480198493433, 0.006921088581216249, 0.009017430114962637, 0.0004780623662079288, 0.028613776406490047, 0.006746746798501871, 0.0020704458689470803, 0.004911353438844664, 0.00996860018515199, 0.009212980855258167, 0.014523371283476806, 0.02696015302641593, 0.02178067729890835, 0.01992015926414904], 5: [0.007551769731635816, 0.0043233083967945075, 0.013755712685054676, 0.015983528905623216, 0.0020186174754032744, 0.000978010933186121, 0.0192795970174455, 0.012148435345866536, 0.01747375212710049, 0.0028734292487765795, 0.001437453851402965, 0.05392536148891055, 0.01146896999414974, 0.02971665193804349, 0.015544074754484752], 6: [0.010712077838429875, 0.000888600246915326, 0.010588617000903014, 0.012535190144262052, 0.0040388735696893115, 0.02573369999819563, 0.00564131166055902, 0.003714967195410561, 0.013740005711357894, 0.012939064086445148, 0.018131500445381175, 0.014630068897717813, 0.016087724430690443, 0.011222635155448432, 0.013609343145914164], 7: [0.027293437461983, 0.0032616412877970588, 0.01591278366940323, 0.01457668052039466, 0.005350908495740907, 0.007194103056659564, 0.015851358727234675, 0.011671089234788526, 0.003991644493198177, 0.0028122987823798315, 0.006852040794176671, 0.011258628569438942, 0.008870095405066928, 0.0030157801898210656, 0.001773027666388688], 8: [0.004122536003643633, 0.013600935117180815, 0.012059676949492391, 0.004836605348209195, 0.008140049272978315, 0.0065721446750501705, 0.010007360177927846, 0.015409404352515881, 0.009816351411476023, 0.010329502179185237, 0.012712097150960281, 0.007004230728924706, 0.001314057978053462, 0.006981535007636191, 0.004191104916861005], 9: [0.007419417896489928, 0.021217124255461677, 0.013951676526649286, 0.0019256029745429847, 0.0008606132151824477, 0.012721454312256289, 0.014570252300385925, 0.00992049556757809, 0.001117535022002027, 0.0017932568433551308, 0.0012357088062270123, 0.000789489613579564, 0.017217400763777313, 0.006743212371511631, 0.012828839578111685]}\n"
     ]
    }
   ],
   "source": [
    "min_max_entropy_diff_index, sum_best_entropy, sum_worst_entropy = find_best_worst_per_class(entropy, entropies_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 0],\n",
       " 1: [10, 2],\n",
       " 2: [9, 10],\n",
       " 3: [7, 6],\n",
       " 4: [4, 5],\n",
       " 5: [5, 11],\n",
       " 6: [1, 5],\n",
       " 7: [14, 0],\n",
       " 8: [12, 7],\n",
       " 9: [11, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_entropy_diff_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.009147526332022043, 0.2910108314774757)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_best_entropy, sum_worst_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We aggregate the most representative subsamples and most unrepresentative subsamples of each class to train CNN and observe the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_best_worst_subsets(grid_subsets, entropy_diff_index):\n",
    "    best_subsets = ConcatDataset([grid_subsets[label][entropy_diff_index[label][0]] for label in grid_subsets])\n",
    "    worst_subsets = ConcatDataset([grid_subsets[label][entropy_diff_index[label][1]] for label in grid_subsets])\n",
    "    return best_subsets, worst_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_agg_subsets, worst_agg_subsets = aggregate_best_worst_subsets(subsamples_per_class, min_max_entropy_diff_index)\n",
    "len(best_agg_subsets), len(worst_agg_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now since we have the most representative and unrepresentative subsets of 5000 images, we can use them to train a CNN submodel respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_size = 5000\n",
    "train_size = len(training_dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(training_dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n",
    "train_loader = DeviceDataLoader(DataLoader(training_dataset, batch_size*2), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.2516, val_loss: 2.1156, val_acc: 0.2048\n",
      "Epoch [1], train_loss: 2.0535, val_loss: 1.9995, val_acc: 0.2532\n",
      "Epoch [2], train_loss: 1.9484, val_loss: 1.9355, val_acc: 0.2608\n",
      "Epoch [3], train_loss: 1.8459, val_loss: 2.0884, val_acc: 0.2511\n",
      "Epoch [4], train_loss: 1.7684, val_loss: 1.7646, val_acc: 0.3345\n",
      "Epoch [5], train_loss: 1.6837, val_loss: 1.7033, val_acc: 0.3529\n",
      "Epoch [6], train_loss: 1.5882, val_loss: 1.7205, val_acc: 0.3798\n",
      "Epoch [7], train_loss: 1.5163, val_loss: 1.4927, val_acc: 0.4412\n",
      "Epoch [8], train_loss: 1.4171, val_loss: 1.9794, val_acc: 0.3595\n",
      "Epoch [9], train_loss: 1.3806, val_loss: 1.5066, val_acc: 0.4356\n",
      "Epoch [10], train_loss: 1.2812, val_loss: 1.5234, val_acc: 0.4585\n",
      "Epoch [11], train_loss: 1.1946, val_loss: 1.3618, val_acc: 0.5064\n",
      "Epoch [12], train_loss: 1.1239, val_loss: 1.3542, val_acc: 0.5098\n",
      "Epoch [13], train_loss: 1.0552, val_loss: 1.4409, val_acc: 0.4906\n",
      "Epoch [14], train_loss: 1.0076, val_loss: 1.4564, val_acc: 0.5070\n",
      "{'val_loss': 1.5256861448287964, 'val_acc': 0.4942382872104645} {'val_loss': 1.4637746810913086, 'val_acc': 0.5129384398460388}\n"
     ]
    }
   ],
   "source": [
    "cifar10bestsub = DeviceDataLoader(DataLoader(best_agg_subsets, batch_size = batch_size, shuffle = True, pin_memory = True), device)\n",
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "best_model = to_device(Cifar10CnnModel(), device)\n",
    "history = fit(num_epochs, lr, best_model, cifar10bestsub, val_dl, opt_func)\n",
    "\n",
    "print(evaluate(best_model, test_loader), evaluate(best_model, train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.2000, val_loss: 2.0648, val_acc: 0.2354\n",
      "Epoch [1], train_loss: 2.0306, val_loss: 1.9801, val_acc: 0.2637\n",
      "Epoch [2], train_loss: 1.9287, val_loss: 1.9137, val_acc: 0.2693\n",
      "Epoch [3], train_loss: 1.7938, val_loss: 1.7456, val_acc: 0.3450\n",
      "Epoch [4], train_loss: 1.6942, val_loss: 1.7970, val_acc: 0.3474\n",
      "Epoch [5], train_loss: 1.5802, val_loss: 1.5736, val_acc: 0.4182\n",
      "Epoch [6], train_loss: 1.4873, val_loss: 1.5404, val_acc: 0.4295\n",
      "Epoch [7], train_loss: 1.3890, val_loss: 1.4947, val_acc: 0.4614\n",
      "Epoch [8], train_loss: 1.2998, val_loss: 1.4780, val_acc: 0.4563\n",
      "Epoch [9], train_loss: 1.2281, val_loss: 1.6579, val_acc: 0.4491\n",
      "Epoch [10], train_loss: 1.1548, val_loss: 1.3855, val_acc: 0.5160\n",
      "Epoch [11], train_loss: 1.0103, val_loss: 1.4861, val_acc: 0.4976\n",
      "Epoch [12], train_loss: 0.9357, val_loss: 1.5349, val_acc: 0.4940\n",
      "Epoch [13], train_loss: 0.7874, val_loss: 1.5272, val_acc: 0.5362\n",
      "Epoch [14], train_loss: 0.6146, val_loss: 1.7607, val_acc: 0.5271\n",
      "{'val_loss': 1.872248649597168, 'val_acc': 0.5003906488418579} {'val_loss': 1.7314767837524414, 'val_acc': 0.5325016379356384}\n"
     ]
    }
   ],
   "source": [
    "cifar10worstsub = DeviceDataLoader(DataLoader(worst_agg_subsets, batch_size = batch_size, shuffle = True, pin_memory = True), device)\n",
    "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "worst_model = to_device(Cifar10CnnModel(), device)\n",
    "history = fit(num_epochs, lr, worst_model, cifar10worstsub, val_dl, opt_func)\n",
    "\n",
    "print(evaluate(worst_model, test_loader), evaluate(worst_model, train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not finished yet, will update later this week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
